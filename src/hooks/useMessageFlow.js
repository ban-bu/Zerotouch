import { useState, useCallback, useRef } from 'react'
import { processWithLLM } from '../services/llmService'

export const useMessageFlow = (currentScenario) => {
  const [messages, setMessages] = useState({
    problem: [],
    llm: [],
    solution: []
  })
  const [llmProcessing, setLlmProcessing] = useState(false)
  const [llmProcessingContext, setLlmProcessingContext] = useState(null) // 'problem' | 'solution' | null
  const [iterationProcessing, setIterationProcessing] = useState(false) // æ–°å¢žï¼šè¿­ä»£å¤„ç†çŠ¶æ€
  const [iterationMode, setIterationMode] = useState(false) // æ–°å¢žï¼šè¿­ä»£æ¨¡å¼çŠ¶æ€
  const [pendingResponse, setPendingResponse] = useState(null) // æ–°å¢žï¼šå¾…å‘é€çš„å“åº”
  
  // æ–°å¢žï¼šéœ€æ±‚åˆ†æžç›¸å…³çŠ¶æ€
  const [missingInfoOptions, setMissingInfoOptions] = useState([])
  const [showMissingInfoPanel, setShowMissingInfoPanel] = useState(false)
  const [currentNeedsAnalysis, setCurrentNeedsAnalysis] = useState(null)

  // æ–°å¢žï¼šæŽ¥å—è¿½é—®åŽç›´å‘å€™é€‰ï¼ˆç”¨äºŽå¯¹æ¯”ç¡®è®¤ï¼‰
  const [directSendCandidate, setDirectSendCandidate] = useState(null)
  
  // æ–°å¢žï¼šAIåä½œå¯¹è¯åŽ†å² - ç‹¬ç«‹ç»´æŠ¤ä¸ŽAIçš„çº¯å¯¹è¯è®°å½•
  const [aiChatHistory, setAiChatHistory] = useState([])

  // æ–°å¢žï¼šèŠå¤©æ¨¡å¼çŠ¶æ€ç®¡ç†
  const [chatMode, setChatMode] = useState('normal') // 'normal', 'department', 'emergency'
  const [departmentModalVisible, setDepartmentModalVisible] = useState(false)
  const [emergencyModalVisible, setEmergencyModalVisible] = useState(false)

  // æ–°å¢žï¼šæµå¼æ˜¾ç¤ºçŠ¶æ€ç®¡ç†
  const [thinkingContent, setThinkingContent] = useState('')
  const [answerContent, setAnswerContent] = useState('')
  const [isStreaming, setIsStreaming] = useState(false)
  
  // æ–°å¢žï¼šAIæŽ¨ç†å¹²é¢„çŠ¶æ€ç®¡ç†
  const [isPaused, setIsPaused] = useState(false)
  const [currentStreamController, setCurrentStreamController] = useState(null)
  const [currentStreamContext, setCurrentStreamContext] = useState(null)

  // æ–°å¢žï¼šåä½œå·¥ä½œå°æµå¼æ¶ˆæ¯çŠ¶æ€ç®¡ç†
  const [streamingMessage, setStreamingMessage] = useState(null) // å½“å‰æ­£åœ¨æµå¼æ˜¾ç¤ºçš„æ¶ˆæ¯
  const [streamingMessageContent, setStreamingMessageContent] = useState('') // æµå¼å†…å®¹
  const streamingMessageRef = useRef(null)
  const currentStreamIdRef = useRef(null)

  // æµå¼å›žè°ƒå‡½æ•°
  const streamCallbacks = {
    onStreamStart: (controller, context) => {
      console.log('ðŸš€ Hook: æµå¼å¼€å§‹å›žè°ƒè§¦å‘')
      setIsStreaming(true)
      setIsPaused(false)
      setThinkingContent('')
      setAnswerContent('')
      setCurrentStreamController(controller)
      setCurrentStreamContext(context)
      
      // ä¸ºåä½œå·¥ä½œå°åˆ›å»ºæµå¼æ¶ˆæ¯å ä½ç¬¦ï¼ˆé™¤éžè¢«ç¦æ­¢ï¼‰
      if (!context?.suppressMessageCreation) {
        const messageType = context?.messageType || 'ai_chat'
        const streamingMsg = {
          type: messageType,
          text: '',
          timestamp: new Date().toISOString(),
          isStreaming: true,
          streamId: `stream_${Date.now()}`,
          // Save chat mode when generated to prevent color changes when switching modes later
          generatedInMode: context?.chatMode || chatMode,
          // å¤åˆ¶ç´§æ€¥å¤„ç†ç›¸å…³å±žæ€§
          ...(messageType === 'emergency_response' && {
            urgencyLevel: context?.urgencyLevel,
            description: context?.description,
            escalated: context?.escalated
          }),
          // å¤åˆ¶æ™ºèƒ½è¿½é—®ç›¸å…³å±žæ€§
          ...(messageType === 'intelligent_followup_candidate' && {
            selectedInfo: context?.selectedInfo,
            feedbackGiven: false,
            accepted: false,
            negotiating: false,
            negotiated: false,
            id: Date.now() + Math.random()
          })
        }
        
        // Add streaming message placeholder to solution panel (first clean old smart follow-up drafts to avoid duplicate drafts)
        setMessages(prev => {
          const cleanedSolution =
            messageType === 'intelligent_followup_candidate'
              ? prev.solution.filter(m => m.type !== 'intelligent_followup_candidate')
              : prev.solution
          return {
            ...prev,
            solution: [...cleanedSolution, streamingMsg]
          }
        })
        
        setStreamingMessage(streamingMsg)
        streamingMessageRef.current = streamingMsg
        setStreamingMessageContent('')
        currentStreamIdRef.current = streamingMsg.streamId
      }
    },
    onThinking: (fragment, fullContent) => {
      console.log('ðŸ§  Hook: æ€è€ƒå†…å®¹æ›´æ–°', {
        fragment: fragment ? fragment.substring(0, 20) + '...' : 'ç©º',
        fullLength: fullContent ? fullContent.length : 0,
        isPaused
      })
      if (!isPaused) {
        setThinkingContent(fullContent)
      }
    },
    onAnswering: (fragment, fullContent) => {
      if (!isPaused) {
        setAnswerContent(fullContent)
        // å®žæ—¶æ›´æ–°åä½œå·¥ä½œå°ä¸­çš„æµå¼æ¶ˆæ¯å†…å®¹ï¼ˆä»…å½“å­˜åœ¨streamIdæ—¶ï¼‰
        const sid = currentStreamIdRef.current
        if (sid) {
          setStreamingMessageContent(fullContent)
          setMessages(prev => ({
            ...prev,
            solution: prev.solution.map(msg => 
              msg.streamId === sid 
                ? { ...msg, text: fullContent, answer: fullContent, isStreaming: true }
                : msg
            )
          }))
        }
      }
    },
    onStreamEnd: (finalThinking, finalAnswer) => {
      console.log('ðŸ Hook: æµå¼ç»“æŸå›žè°ƒè§¦å‘', {
        thinkingLength: finalThinking ? finalThinking.length : 0,
        answerLength: finalAnswer ? finalAnswer.length : 0
      })
      setIsStreaming(false)
      setIsPaused(false)
      setThinkingContent(finalThinking)
      setAnswerContent(finalAnswer)
      setCurrentStreamController(null)
      setCurrentStreamContext(null)
      
      // å®Œæˆæµå¼æ˜¾ç¤ºï¼Œç§»é™¤æµå¼çŠ¶æ€ï¼ˆä»…å½“å­˜åœ¨streamIdæ—¶ï¼‰
      const sid = currentStreamIdRef.current
      if (sid) {
        setMessages(prev => ({
          ...prev,
          solution: prev.solution.map(msg => 
            msg.streamId === sid 
              ? { 
                  ...msg, 
                  text: finalAnswer || msg.text, 
                  isStreaming: false,
                  answer: finalAnswer || msg.answer || msg.text // ä¸ºai_chatç±»åž‹æ·»åŠ answerå­—æ®µ
                }
              : msg
          )
        }))
        // åªæœ‰å½“æœ‰streamIdæ—¶æ‰æ¸…ç†æµå¼çŠ¶æ€
        setStreamingMessage(null)
        streamingMessageRef.current = null
        currentStreamIdRef.current = null
        setStreamingMessageContent('')
      }
    },
    onStreamPaused: () => {
      setIsPaused(true)
    }
  }

  const addMessage = useCallback((panel, message) => {
    setMessages(prev => ({
      ...prev,
      [panel]: [...prev[panel], message]
    }))
  }, [])

  const clearMessages = useCallback(() => {
    setMessages({
      problem: [],
      llm: [],
      solution: []
    })
    setIterationMode(false)
    setPendingResponse(null)
    setLlmProcessing(false)
    setLlmProcessingContext(null)
    // æ¸…ç©ºAIåä½œå¯¹è¯åŽ†å²
    setAiChatHistory([])
    // é‡ç½®èŠå¤©æ¨¡å¼
    setChatMode('normal')
    setDepartmentModalVisible(false)
    setEmergencyModalVisible(false)
    // æ¸…ç©ºæµå¼æ˜¾ç¤ºçŠ¶æ€
    setThinkingContent('')
    setAnswerContent('')
    setIsStreaming(false)
    setIsPaused(false)
    setCurrentStreamController(null)
    setCurrentStreamContext(null)
    // æ¸…ç©ºåä½œå·¥ä½œå°æµå¼çŠ¶æ€
    setStreamingMessage(null)
    setStreamingMessageContent('')
  }, [])

  // æ–°å¢žï¼šAIæŽ¨ç†å¹²é¢„åŠŸèƒ½
  const pauseAI = useCallback(() => {
    if (currentStreamController && currentStreamController.pause) {
      currentStreamController.pause()
      setIsPaused(true)
    }
  }, [currentStreamController])

  const resumeAI = useCallback(() => {
    if (currentStreamController && currentStreamController.resume) {
      currentStreamController.resume()
      setIsPaused(false)
    }
  }, [currentStreamController])

  const adjustAI = useCallback(async (adjustmentText) => {
    if (!currentStreamContext) return
    
    try {
      // åœæ­¢å½“å‰æµå¼å¤„ç†
      if (currentStreamController && currentStreamController.abort) {
        currentStreamController.abort()
      }
      
      console.log('ðŸ”„ åº”ç”¨ç”¨æˆ·è°ƒæ•´å»ºè®®:', adjustmentText)
      
      // æž„å»ºè°ƒæ•´åŽçš„æ¶ˆæ¯
      const adjustmentMessage = {
        role: 'user',
        content: `[ç”¨æˆ·å¹²é¢„è°ƒæ•´] ${adjustmentText}\n\nè¯·æ ¹æ®ä¸Šè¿°å»ºè®®é‡æ–°æ€è€ƒå¹¶è°ƒæ•´æ‚¨çš„æŽ¨ç†è¿‡ç¨‹ã€‚`
      }
      
      // é‡æ–°å¼€å§‹AIå¤„ç†ï¼ŒåŠ å…¥è°ƒæ•´å»ºè®®
      const adjustedContext = {
        ...currentStreamContext,
        messages: [...(currentStreamContext.messages || []), adjustmentMessage]
      }
      
      // é‡æ–°è°ƒç”¨processWithLLM
      const { processWithLLM } = await import('../services/llmService')
      const result = await processWithLLM({
        ...adjustedContext,
        streamCallbacks: streamCallbacks
      })
      
      // æ›´æ–°ç›¸åº”çš„æ¶ˆæ¯
      if (currentStreamContext.onComplete) {
        currentStreamContext.onComplete(result)
      }
      
    } catch (error) {
      console.error('è°ƒæ•´AIæŽ¨ç†æ—¶å‡ºé”™:', error)
      setIsStreaming(false)
      setIsPaused(false)
    }
  }, [currentStreamContext, currentStreamController, streamCallbacks])

  // å‘é€å®¢æˆ·å›žå¤åˆ°é—®é¢˜ç«¯ï¼ˆä¸è§¦å‘è½¬è¯‘ï¼‰
  const sendCustomerReplyToProblem = useCallback((messageData) => {
    const customerReplyMessage = {
      type: 'ai_response', // æ ‡è®°ä¸ºAIå›žå¤ï¼Œä¸æ˜¯ç”¨æˆ·è¾“å…¥
      text: messageData.text,
      timestamp: messageData.timestamp,
      source: 'customer_reply' // æ ‡è®°æ¥æº
    }
    addMessage('problem', customerReplyMessage)
  }, [addMessage])

  const sendProblemMessage = useCallback(async (messageData) => {
    // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°é—®é¢˜ç«¯
    const userMessage = {
      type: 'user',
      text: messageData.text,
      image: messageData.image,
      timestamp: messageData.timestamp
    }
    addMessage('problem', userMessage)

    // å¼€å§‹LLMå¤„ç†ï¼ˆé—®é¢˜ç«¯ â†’ æ–¹æ¡ˆç«¯ï¼‰
    setLlmProcessingContext('problem')
    setLlmProcessing(true)

    try {
      // æž„å»ºå®Œæ•´çš„èŠå¤©åŽ†å² - åŒ…å«æ‰€æœ‰çœŸå®žçš„å¯¹è¯å†…å®¹
      const chatHistory = [
        // é—®é¢˜ç«¯çš„æ‰€æœ‰æ¶ˆæ¯ï¼šç”¨æˆ·è¾“å…¥ + AIä¼˜åŒ–åŽçš„å›žå¤
        ...messages.problem
          .filter(msg => msg.type === 'user' || msg.type === 'ai_response')
          .map(msg => ({ ...msg, panel: 'problem' })),
        // æ–¹æ¡ˆç«¯çš„æ‰€æœ‰æ¶ˆæ¯ï¼šAIè½¬è¯‘çš„è¯·æ±‚ + ä¼ä¸šç”¨æˆ·è¾“å…¥ + AIå›žå¤
        ...messages.solution
          .filter(msg => msg.type === 'llm_request' || msg.type === 'user' || msg.type === 'ai_response')
          .map(msg => ({ ...msg, panel: 'solution' })),
        userMessage // åŒ…å«å½“å‰æ¶ˆæ¯ï¼ˆç”¨æˆ·ï¼‰
      ].sort((a, b) => new Date(a.timestamp) - new Date(b.timestamp))

      // ä½¿ç”¨ç»¼åˆæ™ºèƒ½åˆ†æžï¼ˆéœ€æ±‚åˆ†æž + AIå»ºè®®ï¼‰
      // ä¸ºæœ¬æ¬¡è°ƒç”¨æ³¨å…¥ messageTypeï¼Œç¡®ä¿æµå¼å ä½ç¬¦ä¸º comprehensive_analysisï¼Œé¿å…è¯¯ç”¨ ai_chat
      let usedStreaming = false
      const wrappedStreamCallbacks = {
        ...streamCallbacks,
        onStreamStart: (controller, context) => {
          usedStreaming = true
          if (streamCallbacks && typeof streamCallbacks.onStreamStart === 'function') {
            streamCallbacks.onStreamStart(controller, {
              ...context,
              messageType: 'comprehensive_analysis'
            })
          }
        }
      }

      const llmResult = await processWithLLM({
        type: 'comprehensive_analysis_with_suggestion',
        content: messageData.text,
        image: messageData.image,
        context: 'problem_to_solution',
        scenario: currentScenario,
        chatHistory: chatHistory,
        aiChatHistory: aiChatHistory,
        streamCallbacks: wrappedStreamCallbacks
      })

      // Save current needs analysis results
      setCurrentNeedsAnalysis({
        originalContent: messageData.text,
        image: messageData.image,
        chatHistory: chatHistory
      })

      // è®¾ç½®ç¼ºå¤±ä¿¡æ¯é€‰é¡¹
      setMissingInfoOptions(llmResult.missingInfoOptions || [])
      
      // æ·»åŠ LLMå¤„ç†è¿‡ç¨‹åˆ°ä¸­ä»‹é¢æ¿
      const llmMessage = {
        type: 'processing',
        title: 'Comprehensive analysis',
        steps: [
          {
            name: 'Needs understanding',
            content: llmResult.needsUnderstanding
          },
          {
            name: 'Needs translation',
            content: llmResult.translation
          },
          {
            name: 'AI suggestion',
            content: llmResult.suggestion || 'Generating professional suggestions...'
          },
          {
            name: 'Missing info analysis',
            content: llmResult.missingInfoOptions && llmResult.missingInfoOptions.length > 0 
              ? `Detected ${llmResult.missingInfoOptions.length} info points to collect`
              : 'Information is sufficient; no extra info needed'
          }
        ],
        output: llmResult.suggestion || llmResult.translation,
        timestamp: new Date().toISOString()
      }
      addMessage('llm', llmMessage)

      // æ·»åŠ ç»¼åˆæ™ºèƒ½åˆ†æžç»“æžœåˆ°åä½œå·¥ä½œå°èŠå¤©ç•Œé¢
      // è‹¥æœ¬è½®ä¸ºæµå¼å¤„ç†ï¼Œåˆ™å¤ç”¨åŒä¸€æ¡æµå¼ç´«è‰²æ°”æ³¡ï¼Œå¡«å……æœ€ç»ˆç»“æž„åŒ–å­—æ®µï¼Œé¿å…å‡ºçŽ°ç¬¬äºŒæ¡é‡å¤æ°”æ³¡
      if (!usedStreaming) {
        const comprehensiveMessage = {
          type: 'comprehensive_analysis',
          text: llmResult.needsUnderstanding,
          timestamp: new Date().toISOString(),
          missingInfoOptions: llmResult.missingInfoOptions || [],
          translation: llmResult.translation,
          suggestion: llmResult.suggestion,
          aiAdvice: llmResult.aiAdvice
        }
        addMessage('solution', comprehensiveMessage)
      } else {
        setMessages(prev => {
          const newSolution = [...prev.solution]
          for (let i = newSolution.length - 1; i >= 0; i--) {
            const msg = newSolution[i]
            if (msg && msg.type === 'comprehensive_analysis' && msg.streamId) {
              newSolution[i] = {
                ...msg,
                text: llmResult.needsUnderstanding,
                translation: llmResult.translation,
                suggestion: llmResult.suggestion,
                aiAdvice: llmResult.aiAdvice,
                missingInfoOptions: llmResult.missingInfoOptions || [],
                isStreaming: false
              }
              break
            }
          }
          return { ...prev, solution: newSolution }
        })
      }

      // æ·»åŠ ç¿»è¯‘åŽçš„æ¶ˆæ¯åˆ°æ–¹æ¡ˆç«¯
      const translatedMessage = {
        type: 'llm_request',
        text: llmResult.translation,
        timestamp: new Date().toISOString(),
        needsAnalysis: llmResult.needsUnderstanding,
        missingInfoOptions: llmResult.missingInfoOptions || []
      }
      addMessage('solution', translatedMessage)

      // å¦‚æžœæœ‰ç¼ºå¤±ä¿¡æ¯é€‰é¡¹ï¼Œæ˜¾ç¤ºå‹¾é€‰é¢æ¿
      if (llmResult.missingInfoOptions && llmResult.missingInfoOptions.length > 0) {
        setShowMissingInfoPanel(true)
      }

    } catch (error) {
      console.error('LLMå¤„ç†é”™è¯¯:', error)
      // æ·»åŠ é”™è¯¯æ¶ˆæ¯
      const errorMessage = {
        type: 'processing',
        title: 'Processing error',
        steps: [{
          name: 'Error',
          content: 'Sorry, something went wrong. Please try again later.'
        }],
        timestamp: new Date().toISOString()
      }
      addMessage('llm', errorMessage)
    } finally {
      setLlmProcessing(false)
      setLlmProcessingContext(null)
    }
  }, [addMessage, currentScenario, messages.problem, messages.solution])

  const sendSolutionMessage = useCallback(async (messageData) => {
    // ä¸å†æŠŠåŽŸå§‹è¾“å…¥è¿½åŠ åˆ°æ–¹æ¡ˆç«¯æ¶ˆæ¯ï¼Œä»…ç”¨äºŽä¸Šä¸‹æ–‡
    const userMessage = {
      type: 'user',
      text: messageData.text,
      timestamp: messageData.timestamp
    }

    // éšè—ä¿¡æ¯é€‰æ‹©é¢æ¿ï¼ˆå¦‚æžœæ­£åœ¨æ˜¾ç¤ºï¼‰
    if (showMissingInfoPanel) {
      setShowMissingInfoPanel(false)
      setMissingInfoOptions([])
      setCurrentNeedsAnalysis(null)
    }

    // æ£€æŸ¥æ˜¯å¦æ˜¯åå•†åŽçš„è¿½é—®ã€å·²æŽ¥å—çš„è¿½é—®æˆ–å®¢æˆ·å›žå¤ï¼Œå¦‚æžœæ˜¯åˆ™ç›´æŽ¥å‘é€ï¼Œä¸éœ€è¦AIè½¬è¯‘
    const inputText = messageData.text.trim()
    
    // æ£€æŸ¥åå•†åŽçš„è¿½é—®
    const negotiatedFollowUp = messages.solution.find(msg => 
      (msg.type === 'followup' || msg.type === 'intelligent_followup') && 
      msg.negotiated && 
      msg.text.trim() === inputText
    )

    // æ–°å¢žï¼šæ£€æŸ¥â€œå·²æŽ¥å—çš„è¿½é—®â€ï¼ˆå³ä½¿æœªè¿›å…¥åå•†ï¼‰ï¼Œä¹Ÿèµ°ç›´å‘
    const acceptedFollowUp = messages.solution.find(msg =>
      (msg.type === 'followup' || msg.type === 'intelligent_followup') &&
      msg.feedbackGiven && msg.accepted &&
      (msg.text || '').trim() === inputText
    )

    // æ£€æŸ¥éƒ¨é—¨è”ç»œæŒ‡ä»¤ä¸­çš„å®¢æˆ·å›žå¤
    const customerReplyMatch = messages.solution.find(msg => 
      msg.type === 'department_contact' && 
      msg.customerReply && 
      msg.customerReply.trim() === inputText
    )

    if (negotiatedFollowUp || acceptedFollowUp) {
      console.log('ðŸŽ¯ æ£€æµ‹åˆ°åå•†åŽçš„è¿½é—®ï¼Œç›´æŽ¥å‘é€ç»™ç”¨æˆ·ç«¯ï¼Œè·³è¿‡AIè½¬è¯‘å¤„ç†')
      
      // ç›´æŽ¥å‘é€åˆ°é—®é¢˜ç«¯ï¼Œä¸ç»è¿‡AIè½¬è¯‘
      const directMessage = {
        type: 'ai_response',
        text: inputText,
        timestamp: new Date().toISOString(),
        isNegotiated: !!negotiatedFollowUp // æ ‡è®°æ˜¯å¦ä¸ºåå•†åŽçš„æ¶ˆæ¯
      }
      addMessage('problem', directMessage)

      // æ·»åŠ å¤„ç†è¿‡ç¨‹åˆ°ä¸­ä»‹é¢æ¿ï¼ˆæ˜¾ç¤ºè·³è¿‡è½¬è¯‘ï¼‰
      const skipMessage = {
        type: 'processing',
        title: 'è¿½é—®ç›´è¾¾ç”¨æˆ·ç«¯',
        steps: [{
          name: 'å¤„ç†è¯´æ˜Ž',
          content: 'å·²æŽ¥å—/åå•†å®Œæˆçš„è¿½é—®ç›´æŽ¥å‘é€ç»™ç”¨æˆ·ç«¯ï¼Œæ— éœ€AIäºŒæ¬¡è½¬è¯‘'
        }],
        output: inputText,
        timestamp: new Date().toISOString()
      }
      addMessage('llm', skipMessage)
      
      return // ç›´æŽ¥è¿”å›žï¼Œä¸è¿›è¡ŒåŽç»­çš„AIå¤„ç†
    }

    if (customerReplyMatch) {
      console.log('ðŸŽ¯ æ£€æµ‹åˆ°å®¢æˆ·å›žå¤å†…å®¹ï¼Œç›´æŽ¥å‘é€ç»™ç”¨æˆ·ç«¯ï¼Œè·³è¿‡AIè½¬è¯‘å¤„ç†')
      
      // ç›´æŽ¥å‘é€åˆ°é—®é¢˜ç«¯ï¼Œä¸ç»è¿‡AIè½¬è¯‘
      const directMessage = {
        type: 'ai_response',
        text: inputText,
        timestamp: new Date().toISOString(),
        isCustomerReply: true // æ ‡è®°ä¸ºå®¢æˆ·å›žå¤æ¶ˆæ¯
      }
      addMessage('problem', directMessage)

      // æ·»åŠ å¤„ç†è¿‡ç¨‹åˆ°ä¸­ä»‹é¢æ¿ï¼ˆæ˜¾ç¤ºè·³è¿‡è½¬è¯‘ï¼‰
      const skipMessage = {
        type: 'processing',
        title: 'å®¢æˆ·å›žå¤ç›´è¾¾ç”¨æˆ·ç«¯',
        steps: [{
          name: 'å¤„ç†è¯´æ˜Ž',
          content: 'ç”Ÿæˆçš„å®¢æˆ·å›žå¤ç›´æŽ¥å‘é€ç»™ç”¨æˆ·ç«¯ï¼Œæ— éœ€AIäºŒæ¬¡è½¬è¯‘'
        }],
        output: inputText,
        timestamp: new Date().toISOString()
      }
      addMessage('llm', skipMessage)
      
      return // ç›´æŽ¥è¿”å›žï¼Œä¸è¿›è¡ŒåŽç»­çš„AIå¤„ç†
    }

    // å¼€å§‹LLMå¤„ç†ï¼ˆæ–¹æ¡ˆç«¯ â†’ é—®é¢˜ç«¯ï¼‰
    setLlmProcessingContext('solution')
    setLlmProcessing(true)

    try {
      // æž„å»ºå®Œæ•´çš„èŠå¤©åŽ†å² - åŒ…å«æ‰€æœ‰çœŸå®žçš„å¯¹è¯å†…å®¹
      const chatHistory = [
        // é—®é¢˜ç«¯çš„æ‰€æœ‰æ¶ˆæ¯ï¼šç”¨æˆ·è¾“å…¥ + AIä¼˜åŒ–åŽçš„å›žå¤
        ...messages.problem
          .filter(msg => msg.type === 'user' || msg.type === 'ai_response')
          .map(msg => ({ ...msg, panel: 'problem' })),
        // æ–¹æ¡ˆç«¯çš„æ‰€æœ‰æ¶ˆæ¯ï¼šAIè½¬è¯‘çš„è¯·æ±‚ + ä¼ä¸šç”¨æˆ·è¾“å…¥ + AIå›žå¤
        ...messages.solution
          .filter(msg => msg.type === 'llm_request' || msg.type === 'user' || msg.type === 'ai_response')
          .map(msg => ({ ...msg, panel: 'solution' })),
        userMessage // åŒ…å«å½“å‰æ¶ˆæ¯ï¼ˆä¼ä¸šæ–¹è¾“å…¥ï¼‰
      ].sort((a, b) => new Date(a.timestamp) - new Date(b.timestamp))

      // å¤„ç†æ–¹æ¡ˆç«¯å“åº”
      const llmResult = await processWithLLM({
        type: 'solution_response',
        content: messageData.text,
        context: 'solution_to_problem',
        scenario: currentScenario,
        chatHistory: chatHistory,
        aiChatHistory: aiChatHistory,
        streamCallbacks: streamCallbacks
      })

      // æ·»åŠ LLMå¤„ç†è¿‡ç¨‹åˆ°ä¸­ä»‹é¢æ¿
      const llmMessage = {
        type: 'processing',
        title: 'Optimize reply for customer',
        steps: llmResult.steps,
        output: llmResult.optimizedMessage,
        timestamp: new Date().toISOString()
      }
      addMessage('llm', llmMessage)

      // æ·»åŠ ä¼˜åŒ–åŽçš„å“åº”åˆ°é—®é¢˜ç«¯
      const optimizedMessage = {
        type: 'ai_response',
        text: llmResult.optimizedMessage,
        timestamp: new Date().toISOString()
      }
      addMessage('problem', optimizedMessage)

    } catch (error) {
      console.error('LLMå¤„ç†é”™è¯¯:', error)
      // æ·»åŠ é”™è¯¯æ¶ˆæ¯
      const errorMessage = {
        type: 'processing',
        title: 'Processing error',
        steps: [{
          name: 'Error',
          content: 'Sorry, something went wrong. Please try again later.'
        }],
        timestamp: new Date().toISOString()
      }
      addMessage('llm', errorMessage)
    } finally {
      setLlmProcessing(false)
      setLlmProcessingContext(null)
    }
  }, [addMessage, currentScenario, messages.problem, messages.solution, showMissingInfoPanel])

  // æ–°å¢žï¼šç”Ÿæˆä¼ä¸šç«¯å»ºè®®
  const generateSuggestion = useCallback(async () => {
    if (iterationProcessing) return

    setIterationProcessing(true)

    try {
      // èŽ·å–æœ€æ–°çš„å¯¹è¯å†…å®¹
      const recentMessages = [
        ...messages.problem.filter(m => m.type === 'user' || m.type === 'ai_response').slice(-2),
        ...messages.solution.filter(m => m.type === 'user' || m.type === 'ai_response').slice(-2)
      ].sort((a, b) => new Date(a.timestamp) - new Date(b.timestamp))

      const currentContent = recentMessages.map(msg => msg.text).join('\n')

      // æž„å»ºå®Œæ•´çš„èŠå¤©åŽ†å² - åŒ…å«æ‰€æœ‰çœŸå®žçš„å¯¹è¯å†…å®¹
      const chatHistory = [
        // é—®é¢˜ç«¯çš„æ‰€æœ‰æ¶ˆæ¯ï¼šç”¨æˆ·è¾“å…¥ + AIä¼˜åŒ–åŽçš„å›žå¤
        ...messages.problem
          .filter(msg => msg.type === 'user' || msg.type === 'ai_response')
          .map(msg => ({ ...msg, panel: 'problem' })),
        // æ–¹æ¡ˆç«¯çš„æ‰€æœ‰æ¶ˆæ¯ï¼šAIè½¬è¯‘çš„è¯·æ±‚ + ä¼ä¸šç”¨æˆ·è¾“å…¥ + AIå›žå¤
        ...messages.solution
          .filter(msg => msg.type === 'llm_request' || msg.type === 'user' || msg.type === 'ai_response')
          .map(msg => ({ ...msg, panel: 'solution' }))
      ].sort((a, b) => new Date(a.timestamp) - new Date(b.timestamp))

      // ç”Ÿæˆå»ºè®®
      const llmResult = await processWithLLM({
        type: 'generate_suggestion',
        content: currentContent,
        scenario: currentScenario,
        chatHistory: chatHistory,
        aiChatHistory: aiChatHistory,
        streamCallbacks: streamCallbacks
      })

      // æ·»åŠ LLMå¤„ç†è¿‡ç¨‹åˆ°ä¸­ä»‹é¢æ¿
      const llmMessage = {
        type: 'processing',
        title: 'Generate suggestion',
        steps: llmResult.steps,
        output: llmResult.suggestionMessage,
        structuredOutput: llmResult.structuredOutput,
        timestamp: new Date().toISOString()
      }
      addMessage('llm', llmMessage)

      // å°†å»ºè®®æ·»åŠ åˆ°æ–¹æ¡ˆç«¯ï¼ˆä½œä¸ºè¿­ä»£å†…å®¹ï¼‰
      const suggestionMessage = {
        type: 'suggestion',
        text: llmResult.suggestionMessage,
        timestamp: new Date().toISOString(),
        id: `suggestion_${Date.now()}`,
        feedbackGiven: false
      }
      addMessage('solution', suggestionMessage)

      // è¿›å…¥è¿­ä»£æ¨¡å¼
      setIterationMode(true)
      setPendingResponse(llmResult.suggestionMessage)

    } catch (error) {
      console.error('ç”Ÿæˆå»ºè®®é”™è¯¯:', error)
      const errorMessage = {
        type: 'processing',
        title: 'Suggestion error',
        steps: [{
          name: 'Error',
          content: 'Sorry, failed to generate suggestion. Please try again later.'
        }],
        timestamp: new Date().toISOString()
      }
      addMessage('llm', errorMessage)
    } finally {
      setIterationProcessing(false)
    }
  }, [addMessage, currentScenario, messages.problem, messages.solution, iterationProcessing])

  // æ–°å¢žï¼šç”Ÿæˆä¼ä¸šç«¯è¿½é—®
  const generateFollowUp = useCallback(async () => {
    if (iterationProcessing) return

    setIterationProcessing(true)

    try {
      // èŽ·å–æœ€æ–°çš„å¯¹è¯å†…å®¹
      const recentMessages = [
        ...messages.problem.filter(m => m.type === 'user' || m.type === 'ai_response').slice(-2),
        ...messages.solution.filter(m => m.type === 'user' || m.type === 'ai_response').slice(-2)
      ].sort((a, b) => new Date(a.timestamp) - new Date(b.timestamp))

      const currentContent = recentMessages.map(msg => msg.text).join('\n')

      // æž„å»ºå®Œæ•´çš„èŠå¤©åŽ†å² - åŒ…å«æ‰€æœ‰çœŸå®žçš„å¯¹è¯å†…å®¹
      const chatHistory = [
        // é—®é¢˜ç«¯çš„æ‰€æœ‰æ¶ˆæ¯ï¼šç”¨æˆ·è¾“å…¥ + AIä¼˜åŒ–åŽçš„å›žå¤
        ...messages.problem
          .filter(msg => msg.type === 'user' || msg.type === 'ai_response')
          .map(msg => ({ ...msg, panel: 'problem' })),
        // æ–¹æ¡ˆç«¯çš„æ‰€æœ‰æ¶ˆæ¯ï¼šAIè½¬è¯‘çš„è¯·æ±‚ + ä¼ä¸šç”¨æˆ·è¾“å…¥ + AIå›žå¤
        ...messages.solution
          .filter(msg => msg.type === 'llm_request' || msg.type === 'user' || msg.type === 'ai_response')
          .map(msg => ({ ...msg, panel: 'solution' }))
      ].sort((a, b) => new Date(a.timestamp) - new Date(b.timestamp))

      // ç”Ÿæˆè¿½é—®
      const llmResult = await processWithLLM({
        type: 'generate_followup',
        content: currentContent,
        scenario: currentScenario,
        chatHistory: chatHistory,
        aiChatHistory: aiChatHistory,
        streamCallbacks: streamCallbacks
      })

      // æ·»åŠ LLMå¤„ç†è¿‡ç¨‹åˆ°ä¸­ä»‹é¢æ¿
      const llmMessage = {
        type: 'processing',
        title: 'Generate follow-up',
        steps: llmResult.steps,
        output: llmResult.followUpMessage,
        structuredOutput: llmResult.structuredOutput,
        timestamp: new Date().toISOString()
      }
      addMessage('llm', llmMessage)

      // å°†è¿½é—®æ·»åŠ åˆ°æ–¹æ¡ˆç«¯ï¼ˆä½œä¸ºè¿­ä»£å†…å®¹ï¼‰
      const followUpMessage = {
        type: 'followup',
        text: llmResult.followUpMessage,
        timestamp: new Date().toISOString(),
        id: `followup_${Date.now()}`,
        feedbackGiven: false
      }
      addMessage('solution', followUpMessage)

      // è¿›å…¥è¿­ä»£æ¨¡å¼
      setIterationMode(true)
      setPendingResponse(llmResult.followUpMessage)

    } catch (error) {
      console.error('ç”Ÿæˆè¿½é—®é”™è¯¯:', error)
      const errorMessage = {
        type: 'processing',
        title: 'Follow-up error',
        steps: [{
          name: 'Error',
          content: 'Sorry, failed to generate follow-up. Please try again later.'
        }],
        timestamp: new Date().toISOString()
      }
      addMessage('llm', errorMessage)
    } finally {
      setIterationProcessing(false)
    }
  }, [addMessage, currentScenario, messages.problem, messages.solution, iterationProcessing])

  // æ–°å¢žï¼šç¡®è®¤å‘é€æœ€ç»ˆå“åº”
  const confirmSendResponse = useCallback(async (finalResponse) => {
    if (llmProcessing) return

    // é¦–å…ˆæ·»åŠ ç”¨æˆ·çš„æœ€ç»ˆå“åº”æ¶ˆæ¯åˆ°æ–¹æ¡ˆç«¯
    const userFinalMessage = {
      type: 'user',
      text: finalResponse,
      timestamp: new Date().toISOString()
    }
    addMessage('solution', userFinalMessage)

    setLlmProcessingContext('solution')
    setLlmProcessing(true)

    try {
      // æž„å»ºå®Œæ•´çš„èŠå¤©åŽ†å² - åŒ…å«æ‰€æœ‰çœŸå®žçš„å¯¹è¯å†…å®¹
      const chatHistory = [
        // é—®é¢˜ç«¯çš„æ‰€æœ‰æ¶ˆæ¯ï¼šç”¨æˆ·è¾“å…¥ + AIä¼˜åŒ–åŽçš„å›žå¤
        ...messages.problem
          .filter(msg => msg.type === 'user' || msg.type === 'ai_response')
          .map(msg => ({ ...msg, panel: 'problem' })),
        // æ–¹æ¡ˆç«¯çš„æ‰€æœ‰æ¶ˆæ¯ï¼šAIè½¬è¯‘çš„è¯·æ±‚ + ä¼ä¸šç”¨æˆ·è¾“å…¥ + AIå›žå¤
        ...messages.solution
          .filter(msg => msg.type === 'llm_request' || msg.type === 'user' || msg.type === 'ai_response')
          .map(msg => ({ ...msg, panel: 'solution' })),
        userFinalMessage // åŒ…å«ç”¨æˆ·çš„æœ€ç»ˆå“åº”
      ].sort((a, b) => new Date(a.timestamp) - new Date(b.timestamp))

      // å¤„ç†æœ€ç»ˆå“åº”
      const llmResult = await processWithLLM({
        type: 'solution_response',
        content: finalResponse,
        context: 'solution_to_problem',
        scenario: currentScenario,
        chatHistory: chatHistory,
        aiChatHistory: aiChatHistory,
        streamCallbacks: streamCallbacks
      })

      // æ·»åŠ LLMå¤„ç†è¿‡ç¨‹åˆ°ä¸­ä»‹é¢æ¿
      const llmMessage = {
        type: 'processing',
        title: 'Process final response',
        steps: llmResult.steps,
        output: llmResult.optimizedMessage,
        timestamp: new Date().toISOString()
      }
      addMessage('llm', llmMessage)

      // æ·»åŠ ä¼˜åŒ–åŽçš„å“åº”åˆ°é—®é¢˜ç«¯
      const optimizedMessage = {
        type: 'ai_response',
        text: llmResult.optimizedMessage,
        timestamp: new Date().toISOString()
      }
      addMessage('problem', optimizedMessage)

      // é€€å‡ºè¿­ä»£æ¨¡å¼
      setIterationMode(false)
      setPendingResponse(null)

    } catch (error) {
      console.error('ç¡®è®¤å‘é€é”™è¯¯:', error)
      const errorMessage = {
        type: 'processing',
        title: 'Final response error',
        steps: [{
          name: 'Error',
          content: 'Sorry, failed to process final response. Please try again later.'
        }],
        timestamp: new Date().toISOString()
      }
      addMessage('llm', errorMessage)
    } finally {
      setLlmProcessing(false)
      setLlmProcessingContext(null)
    }
  }, [addMessage, currentScenario, messages.problem, messages.solution, llmProcessing])

  // New: Cancel iteration mode
  const cancelIteration = useCallback(() => {
    setIterationMode(false)
    setPendingResponse(null)
  }, [])

  // æ–°å¢žï¼šå¤„ç†ä¿¡æ¯é€‰é¡¹å‹¾é€‰
  const toggleMissingInfoOption = useCallback((index) => {
    setMissingInfoOptions(prev => 
      prev.map((option, i) => 
        i === index ? { ...option, selected: !option.selected } : option
      )
    )
  }, [])

  // æ–°å¢žï¼šç”ŸæˆåŸºäºŽé€‰ä¸­ä¿¡æ¯çš„è¿½é—®
  const generateFollowUpBySelectedInfo = useCallback(async () => {
    // ä¸¥æ ¼æ£€æŸ¥ï¼šé˜²æ­¢é‡å¤è°ƒç”¨
    if (!currentNeedsAnalysis || iterationProcessing) {
      console.log('âš ï¸ generateFollowUpBySelectedInfo: æ¡ä»¶ä¸æ»¡è¶³ï¼Œè·³è¿‡æ‰§è¡Œ', {
        hasCurrentNeedsAnalysis: !!currentNeedsAnalysis,
        iterationProcessing
      })
      return
    }
    
    const selectedOptions = missingInfoOptions.filter(option => option.selected)
    if (selectedOptions.length === 0) {
      console.log('âš ï¸ generateFollowUpBySelectedInfo: æ²¡æœ‰é€‰ä¸­çš„é€‰é¡¹ï¼Œè·³è¿‡æ‰§è¡Œ')
      return
    }

    console.log('ðŸ”„ å¼€å§‹ç”Ÿæˆæ™ºèƒ½è¿½é—®ï¼Œé€‰ä¸­é€‰é¡¹æ•°é‡:', selectedOptions.length)
    setIterationProcessing(true)
    
    // ç«‹å³éšè—é¢æ¿ï¼Œé˜²æ­¢é‡å¤ç‚¹å‡»
    setShowMissingInfoPanel(false)

    // Before generating new draft, clean old smart follow-up drafts to ensure no duplicate drafts appear
    setMessages(prev => ({
      ...prev,
      solution: prev.solution.filter(m => m.type !== 'intelligent_followup_candidate')
    }))

    try {
      // è®¾ç½®æµå¼ä¸Šä¸‹æ–‡ï¼ŒæŒ‡å®šæ¶ˆæ¯ç±»åž‹ä¸ºæ™ºèƒ½è¿½é—®
      const intelligentFollowUpStreamCallbacks = {
        ...streamCallbacks,
        onStreamStart: (controller, context) => {
          if (streamCallbacks && streamCallbacks.onStreamStart) {
            streamCallbacks.onStreamStart(controller, {
              ...context,
              messageType: 'intelligent_followup_candidate',
              selectedInfo: selectedOptions
            })
          }
        }
      }

      // ç”Ÿæˆè¿½é—®ï¼ˆæµå¼å›žè°ƒä¼šè‡ªåŠ¨åˆ›å»ºæ¶ˆæ¯ï¼Œæ— éœ€æ‰‹åŠ¨addMessageï¼‰
      const llmResult = await processWithLLM({
        type: 'generate_questions_by_selected_info',
        content: {
          originalContent: currentNeedsAnalysis.originalContent,
          selectedInfoItems: selectedOptions
        },
        scenario: currentScenario,
        chatHistory: currentNeedsAnalysis.chatHistory,
        aiChatHistory: aiChatHistory,
        streamCallbacks: intelligentFollowUpStreamCallbacks
      })

      // è‹¥è¿›è¡Œäº†éžæµå¼ä¸¥æ ¼é‡è¯•ï¼Œåˆ™æ­¤å¤„è¿”å›žçš„æ˜¯æœ€ç»ˆç»“æžœå­—ç¬¦ä¸²
      // ä¸ºé¿å…å‡ºçŽ°â€œåˆæ¬¡æµå¼ç»“æžœ + é‡è¯•ç»“æžœâ€ä¸¤æ¡æ°”æ³¡ï¼Œè¿™é‡Œç»Ÿä¸€å°†æœ€æ–°ç»“æžœå›žå†™åˆ°æœ€è¿‘çš„å€™é€‰æ°”æ³¡ä¸­
      if (typeof llmResult === 'string' && llmResult.trim()) {
        setMessages(prev => {
          const newSolution = [...prev.solution]
          for (let i = newSolution.length - 1; i >= 0; i--) {
            const msg = newSolution[i]
            if (msg && msg.type === 'intelligent_followup_candidate') {
              newSolution[i] = { ...msg, text: llmResult.trim(), isStreaming: false }
              break
            }
          }
          return { ...prev, solution: newSolution }
        })
      }

      // æ·»åŠ LLMå¤„ç†è¿‡ç¨‹åˆ°ä¸­ä»‹é¢æ¿
      const llmMessage = {
        type: 'processing',
        title: 'ç”Ÿæˆæ™ºèƒ½è¿½é—®',
        steps: [
          {
            name: 'é€‰ä¸­ä¿¡æ¯',
            content: selectedOptions.map(opt => `${opt.name}ï¼š${opt.description}`).join('\n')
          },
          {
            name: 'ç”Ÿæˆè¿½é—®',
            content: llmResult
          }
        ],
        output: llmResult,
        timestamp: new Date().toISOString()
      }
      addMessage('llm', llmMessage)
      console.log('âœ… æ™ºèƒ½è¿½é—®ç”Ÿæˆå®Œæˆ')

    } catch (error) {
      console.error('ç”Ÿæˆè¿½é—®é”™è¯¯:', error)
      const errorMessage = {
        type: 'processing',
        title: 'ç”Ÿæˆè¿½é—®å‡ºé”™',
        steps: [{
          name: 'é”™è¯¯ä¿¡æ¯',
          content: 'æŠ±æ­‰ï¼Œç”Ÿæˆè¿½é—®æ—¶å‡ºçŽ°äº†é”™è¯¯ï¼Œè¯·ç¨åŽé‡è¯•ã€‚'
        }],
        timestamp: new Date().toISOString()
      }
      addMessage('llm', errorMessage)
      
      // å‘ç”Ÿé”™è¯¯æ—¶é‡æ–°æ˜¾ç¤ºé¢æ¿ï¼Œè®©ç”¨æˆ·å¯ä»¥é‡è¯•
      setShowMissingInfoPanel(true)
    } finally {
      setIterationProcessing(false)
    }
  }, [currentNeedsAnalysis, missingInfoOptions, currentScenario, iterationProcessing, addMessage, showMissingInfoPanel])

  // æ–°å¢žï¼šè·³è¿‡ä¿¡æ¯æ”¶é›†ï¼Œç›´æŽ¥å›žå¤
  const skipInfoCollection = useCallback(() => {
    setShowMissingInfoPanel(false)
    setMissingInfoOptions([])
    setCurrentNeedsAnalysis(null)
  }, [])

  // æ–°å¢žï¼šç®€å•çš„æ™ºèƒ½è¿½é—®ç”Ÿæˆï¼ˆåŸºäºŽå½“å‰å¯¹è¯ï¼‰
  const generateSimpleIntelligentFollowUp = useCallback(async () => {
    // ä¸¥æ ¼æ£€æŸ¥ï¼šé˜²æ­¢ä¸Žå…¶ä»–è¿½é—®ç”Ÿæˆå†²çª
    if (llmProcessing || iterationProcessing) {
      console.log('âš ï¸ generateSimpleIntelligentFollowUp: System is processing, skipping execution')
      return
    }
    
    // å¦‚æžœä¿¡æ¯é€‰æ‹©é¢æ¿æ­£åœ¨æ˜¾ç¤ºï¼Œä¼˜å…ˆä½¿ç”¨é€‰æ‹©å¼è¿½é—®
    if (showMissingInfoPanel) {
      console.log('âš ï¸ generateSimpleIntelligentFollowUp: ä¿¡æ¯é€‰æ‹©é¢æ¿å·²æ˜¾ç¤ºï¼Œè·³è¿‡ç®€å•è¿½é—®ç”Ÿæˆ')
      return
    }
    
    console.log('ðŸ”„ å¼€å§‹ç”Ÿæˆç®€å•æ™ºèƒ½è¿½é—®')
    setIterationProcessing(true)
    
    try {
      // èŽ·å–æœ€è¿‘çš„å¯¹è¯å†…å®¹
      const recentProblemMessages = messages.problem.slice(-3)
      const recentSolutionMessages = messages.solution.slice(-3)
      
      const content = [...recentProblemMessages, ...recentSolutionMessages]
        .sort((a, b) => new Date(a.timestamp) - new Date(b.timestamp))
        .map(msg => msg.text || msg.output)
        .join('\n')
      
      if (!content.trim()) {
        console.log('æ²¡æœ‰è¶³å¤Ÿçš„å¯¹è¯å†…å®¹ç”Ÿæˆæ™ºèƒ½è¿½é—®')
        setIterationProcessing(false)
        return
      }

      // è°ƒç”¨LLMç”Ÿæˆæ™ºèƒ½è¿½é—® - ç®€å•è¿½é—®ä¸ä½¿ç”¨æµå¼å›žè°ƒï¼Œåªåˆ›å»ºLLMè®°å½•
      const llmResult = await processWithLLM({
        type: 'generate_simple_followup',
        content: content,
        scenario: currentScenario,
        chatHistory: [...recentProblemMessages, ...recentSolutionMessages],
        aiChatHistory: aiChatHistory,
        streamCallbacks: null // ç®€å•è¿½é—®ä¸åˆ›å»ºsolutioné¢æ¿æ¶ˆæ¯ï¼Œåªè®°å½•åˆ°LLMé¢æ¿
      })

      // æ·»åŠ åˆ°LLMé¢æ¿æ˜¾ç¤º
      const llmMessage = {
        type: 'processing',
        title: 'ç”Ÿæˆæ™ºèƒ½è¿½é—®',
        steps: llmResult.steps,
        output: llmResult.followUpMessage,
        timestamp: new Date().toISOString()
      }
      addMessage('llm', llmMessage)

      console.log('âœ… æ™ºèƒ½è¿½é—®ç”Ÿæˆå®Œæˆ:', llmResult.followUpMessage)
      
    } catch (error) {
      console.error('ç”Ÿæˆæ™ºèƒ½è¿½é—®é”™è¯¯:', error)
    } finally {
      setIterationProcessing(false)
    }
  }, [llmProcessing, iterationProcessing, messages.problem, messages.solution, currentScenario, addMessage, showMissingInfoPanel])

  // æ–°å¢žï¼šç‹¬ç«‹çš„æ™ºèƒ½éœ€æ±‚åˆ†æžï¼ˆåŸºäºŽå½“å‰å¯¹è¯ï¼‰
  const generateIntelligentNeedsAnalysis = useCallback(async () => {
    if (llmProcessing || iterationProcessing) return
    
    setIterationProcessing(true)
    
    try {
      // èŽ·å–æœ€è¿‘çš„é—®é¢˜ç«¯æ¶ˆæ¯ï¼ˆç”¨æˆ·è¾“å…¥ï¼‰
      const recentProblemMessages = messages.problem.filter(msg => msg.type === 'user').slice(-2)
      
      if (recentProblemMessages.length === 0) {
        console.log('æ²¡æœ‰ç”¨æˆ·è¾“å…¥å†…å®¹è¿›è¡Œéœ€æ±‚åˆ†æž')
        setIterationProcessing(false)
        return
      }

      // ä½¿ç”¨æœ€æ–°çš„ç”¨æˆ·è¾“å…¥è¿›è¡Œåˆ†æž
      const latestUserMessage = recentProblemMessages[recentProblemMessages.length - 1]
      
      // æž„å»ºèŠå¤©åŽ†å²
      const chatHistory = [
        ...messages.problem.slice(-3),
        ...messages.solution.slice(-3)
      ].sort((a, b) => new Date(a.timestamp) - new Date(b.timestamp))

      // è°ƒç”¨æ™ºèƒ½éœ€æ±‚åˆ†æž
      const llmResult = await processWithLLM({
        type: 'analyze_needs_with_missing_info',
        content: latestUserMessage.text,
        image: latestUserMessage.image,
        scenario: currentScenario,
        chatHistory: chatHistory,
        aiChatHistory: aiChatHistory,
        streamCallbacks: streamCallbacks
      })

      // æ·»åŠ åˆ°LLMé¢æ¿æ˜¾ç¤ºå¤„ç†è¿‡ç¨‹
      const llmMessage = {
        type: 'processing',
        title: 'æ™ºèƒ½éœ€æ±‚åˆ†æž',
        steps: [
          {
            name: 'éœ€æ±‚ç†è§£',
            content: llmResult.needsUnderstanding
          },
          {
            name: 'ä¿¡æ¯é€‰é¡¹',
            content: llmResult.missingInfoOptions.map(opt => `${opt.name}ï¼š${opt.description}`).join('\n')
          },
          {
            name: 'éœ€æ±‚è½¬è¯‘',
            content: llmResult.translation
          }
        ],
        output: llmResult.needsUnderstanding,
        timestamp: new Date().toISOString(),
        structuredOutput: llmResult.structuredOutput
      }
      addMessage('llm', llmMessage)

      // æ·»åŠ æ™ºèƒ½éœ€æ±‚åˆ†æžç»“æžœåˆ°åä½œå·¥ä½œå°èŠå¤©ç•Œé¢
      const analysisMessage = {
        type: 'needs_analysis',
        text: llmResult.needsUnderstanding,
        timestamp: new Date().toISOString(),
        missingInfoOptions: llmResult.missingInfoOptions || [],
        translation: llmResult.translation
      }
      addMessage('solution', analysisMessage)

      // è®¾ç½®ç¼ºå¤±ä¿¡æ¯é€‰é¡¹çŠ¶æ€
      setMissingInfoOptions(llmResult.missingInfoOptions || [])
      if (llmResult.missingInfoOptions && llmResult.missingInfoOptions.length > 0) {
        setShowMissingInfoPanel(true)
      }

      console.log('âœ… æ™ºèƒ½éœ€æ±‚åˆ†æžå®Œæˆ:', llmResult)
      
    } catch (error) {
      console.error('æ™ºèƒ½éœ€æ±‚åˆ†æžé”™è¯¯:', error)
    } finally {
      setIterationProcessing(false)
    }
  }, [llmProcessing, iterationProcessing, messages.problem, messages.solution, currentScenario, addMessage])

  // æ–°å¢žï¼šæŽ¥å—å»ºè®®
  const acceptSuggestion = useCallback((suggestionId) => {
    setMessages(prev => ({
      ...prev,
      solution: prev.solution.map(msg => 
        msg.id === suggestionId 
          ? { ...msg, feedbackGiven: true, accepted: true }
          : msg
      )
    }))
  }, [])

  // æ–°å¢žï¼šä¸ŽAIåå•†å»ºè®®
  const negotiateSuggestion = useCallback((suggestionId) => {
    // æ ‡è®°å»ºè®®è¿›å…¥åå•†æ¨¡å¼
    setMessages(prev => ({
      ...prev,
      solution: prev.solution.map(msg => 
        msg.id === suggestionId 
          ? { ...msg, negotiating: true }
          : msg
      )
    }))
  }, [])

  // New: Cancel negotiation mode
  const cancelNegotiation = useCallback((suggestionId) => {
    setMessages(prev => ({
      ...prev,
      solution: prev.solution.map(msg => 
        msg.id === suggestionId 
          ? { ...msg, negotiating: false }
          : msg
      )
    }))
  }, [])

  // New: Send negotiation request
  const sendNegotiationRequest = useCallback(async (suggestionId, negotiationText, onUpdateContent) => {
    if (!negotiationText.trim()) return

    console.log('ðŸ”„ å¼€å§‹å¤„ç†å»ºè®®åå•†è¯·æ±‚:', { suggestionId, negotiationText })

    try {
      // æ ¹æ®messageIdæ ¼å¼åˆ¤æ–­æ˜¯ä»Žå“ªä¸ªæ¶ˆæ¯æ•°ç»„æŸ¥æ‰¾
      let originalSuggestion
      if (suggestionId.includes('_suggestion')) {
        // è¿™æ˜¯æ¥è‡ªLLMé¢æ¿çš„æ¶ˆæ¯ï¼Œä»Žmessages.llmä¸­æŸ¥æ‰¾
        const messageIndex = parseInt(suggestionId.split('_')[0])
        originalSuggestion = messages.llm[messageIndex]
      } else {
        // è¿™æ˜¯æ¥è‡ªsolutioné¢æ¿çš„æ¶ˆæ¯
        originalSuggestion = messages.solution.find(msg => msg.id === suggestionId)
      }
      
      if (!originalSuggestion) {
        console.error('âŒ æœªæ‰¾åˆ°åŽŸå§‹å»ºè®®', { 
          suggestionId, 
          messagesLlmLength: messages.llm.length,
          messagesSolutionLength: messages.solution.length 
        })
        return
      }
      
      console.log('âœ… æ‰¾åˆ°åŽŸå§‹å»ºè®®:', { 
        suggestionId, 
        originalSuggestionText: originalSuggestion.text || originalSuggestion.output 
      })

      // æž„å»ºåå•†ä¸Šä¸‹æ–‡
      const chatHistory = [
        ...messages.problem
          .filter(msg => msg.type === 'user' || msg.type === 'ai_response')
          .map(msg => ({ ...msg, panel: 'problem' })),
        ...messages.solution
          .filter(msg => msg.type === 'llm_request' || msg.type === 'user' || msg.type === 'ai_response')
          .map(msg => ({ ...msg, panel: 'solution' }))
      ].sort((a, b) => new Date(a.timestamp) - new Date(b.timestamp))

      // ç”Ÿæˆåå•†åŽçš„å»ºè®®
      const llmResult = await processWithLLM({
        type: 'negotiate_suggestion',
        content: {
          originalSuggestion: originalSuggestion.text || originalSuggestion.output,
          negotiationRequest: negotiationText,
          negotiationHistory: originalSuggestion.negotiationHistory || []
        },
        scenario: currentScenario,
        chatHistory: chatHistory,
        aiChatHistory: aiChatHistory,
        streamCallbacks: streamCallbacks
      })

      // ä¸å†æ·»åŠ å¤„ç†è¯´æ˜Žåˆ°LLMé¢æ¿ï¼Œç›´æŽ¥æ›´æ–°åŽŸå†…å®¹

      // è°ƒç”¨å›žè°ƒå‡½æ•°æ›´æ–°æ˜¾ç¤ºå†…å®¹
      if (onUpdateContent && typeof onUpdateContent === 'function') {
        console.log('ðŸ”„ è°ƒç”¨å›žè°ƒå‡½æ•°æ›´æ–°å»ºè®®å†…å®¹:', llmResult.suggestionMessage)
        onUpdateContent(llmResult.suggestionMessage)
      } else {
        console.warn('âš ï¸ æœªæä¾›å›žè°ƒå‡½æ•°æˆ–å›žè°ƒå‡½æ•°æ— æ•ˆ')
      }
      
      console.log('âœ… å»ºè®®åå•†å¤„ç†å®Œæˆ')

      // æ›´æ–°åŽŸå»ºè®®ä¸ºåå•†åŽçš„ç‰ˆæœ¬ï¼Œä¿ç•™åå•†åŽ†å²
      setMessages(prev => ({
        ...prev,
        solution: prev.solution.map(msg => 
          msg.id === suggestionId 
            ? { 
                ...msg, 
                text: llmResult.suggestionMessage,
                negotiating: false,
                negotiated: true,
                negotiationHistory: [
                  ...(msg.negotiationHistory || []),
                  {
                    previousText: msg.negotiationHistory?.length > 0 ? msg.text : (msg.originalText || msg.text),
                    negotiationRequest: negotiationText,
                    timestamp: new Date().toISOString()
                  }
                ],
                originalText: msg.originalText || originalSuggestion.text
              }
            : msg
        )
      }))

    } catch (error) {
      console.error('åå•†å»ºè®®é”™è¯¯:', error)
      // åå•†å¤±è´¥ï¼Œé€€å‡ºåå•†æ¨¡å¼
      cancelNegotiation(suggestionId)
    }
  }, [messages.problem, messages.solution, currentScenario, addMessage, cancelNegotiation])

  // æ–°å¢žï¼šæ‹’ç»å»ºè®®å¹¶é‡æ–°ç”Ÿæˆ
  const rejectSuggestion = useCallback(async (suggestionId) => {
    // æ ‡è®°å»ºè®®ä¸ºå·²æ‹’ç»
    setMessages(prev => ({
      ...prev,
      solution: prev.solution.map(msg => 
        msg.id === suggestionId 
          ? { ...msg, feedbackGiven: true, accepted: false }
          : msg
      )
    }))

    // é‡æ–°ç”Ÿæˆå»ºè®®
    await generateSuggestion()
  }, [generateSuggestion])

  // æ–°å¢žï¼šæŽ¥å—è¿½é—®
  const acceptFollowUp = useCallback((followUpId, onSetInput) => {
    const followUpMessage = messages.solution.find(msg => msg.id === followUpId)
    if (!followUpMessage) return

    // æ ‡è®°ä¸ºå·²æŽ¥å—
    setMessages(prev => ({
      ...prev,
      solution: prev.solution.map(msg => 
        msg.id === followUpId 
          ? { ...msg, feedbackGiven: true, accepted: true }
          : msg
      )
    }))

    // å°†è¿½é—®å†…å®¹å¡«å…¥è¾“å…¥æ¡†
    if (onSetInput && typeof onSetInput === 'function') {
      onSetInput(followUpMessage.text)
    }

    // è®¾ä¸ºç›´å‘å€™é€‰ï¼Œç­‰å¾…å¯¹æ¯”ç¡®è®¤
    setDirectSendCandidate({
      type: 'followup',
      sourceMessageId: followUpId,
      sourceText: followUpMessage.text,
      createdAt: new Date().toISOString()
    })
  }, [messages.solution])

  // æ–°å¢žï¼šä¸ŽAIåå•†è¿½é—®
  const negotiateFollowUp = useCallback((followUpId) => {
    // æ ‡è®°è¿½é—®è¿›å…¥åå•†æ¨¡å¼
    setMessages(prev => ({
      ...prev,
      solution: prev.solution.map(msg => 
        msg.id === followUpId 
          ? { ...msg, negotiating: true }
          : msg
      )
    }))
  }, [])

  // New: Cancel follow-up negotiation mode
  const cancelFollowUpNegotiation = useCallback((followUpId) => {
    setMessages(prev => ({
      ...prev,
      solution: prev.solution.map(msg => 
        msg.id === followUpId 
          ? { ...msg, negotiating: false }
          : msg
      )
    }))
  }, [])

  // æ–°å¢žï¼šå‘é€è¿½é—®åå•†è¯·æ±‚
  const sendFollowUpNegotiationRequest = useCallback(async (followUpId, negotiationText, onUpdateContent) => {
    if (!negotiationText.trim()) return

    console.log('ðŸ”„ å¼€å§‹å¤„ç†è¿½é—®åå•†è¯·æ±‚:', { followUpId, negotiationText })

    try {
      // æ ¹æ®messageIdæ ¼å¼åˆ¤æ–­æ˜¯ä»Žå“ªä¸ªæ¶ˆæ¯æ•°ç»„æŸ¥æ‰¾
      let originalFollowUp
      if (followUpId.includes('_followup')) {
        // è¿™æ˜¯æ¥è‡ªLLMé¢æ¿çš„æ¶ˆæ¯ï¼Œä»Žmessages.llmä¸­æŸ¥æ‰¾
        const messageIndex = parseInt(followUpId.split('_')[0])
        originalFollowUp = messages.llm[messageIndex]
      } else {
        // è¿™æ˜¯æ¥è‡ªsolutioné¢æ¿çš„æ¶ˆæ¯
        originalFollowUp = messages.solution.find(msg => msg.id === followUpId)
      }
      
      if (!originalFollowUp) {
        console.error('æœªæ‰¾åˆ°åŽŸå§‹è¿½é—®ï¼ŒfollowUpId:', followUpId)
        return
      }

      // æž„å»ºåå•†ä¸Šä¸‹æ–‡
      const chatHistory = [
        ...messages.problem
          .filter(msg => msg.type === 'user' || msg.type === 'ai_response')
          .map(msg => ({ ...msg, panel: 'problem' })),
        ...messages.solution
          .filter(msg => msg.type === 'llm_request' || msg.type === 'user' || msg.type === 'ai_response')
          .map(msg => ({ ...msg, panel: 'solution' }))
      ].sort((a, b) => new Date(a.timestamp) - new Date(b.timestamp))

      // ç”Ÿæˆåå•†åŽçš„è¿½é—®
      const llmResult = await processWithLLM({
        type: 'negotiate_followup',
        content: {
          originalFollowUp: originalFollowUp.text || originalFollowUp.output,
          negotiationRequest: negotiationText,
          negotiationHistory: originalFollowUp.negotiationHistory || []
        },
        scenario: currentScenario,
        chatHistory: chatHistory,
        aiChatHistory: aiChatHistory,
        streamCallbacks: streamCallbacks
      })

      // ä¸å†æ·»åŠ å¤„ç†è¯´æ˜Žåˆ°LLMé¢æ¿ï¼Œç›´æŽ¥æ›´æ–°åŽŸå†…å®¹

      // è°ƒç”¨å›žè°ƒå‡½æ•°æ›´æ–°æ˜¾ç¤ºå†…å®¹
      if (onUpdateContent && typeof onUpdateContent === 'function') {
        onUpdateContent(llmResult.followUpMessage)
      }

      // æ›´æ–°åŽŸè¿½é—®ä¸ºåå•†åŽçš„ç‰ˆæœ¬ï¼Œä¿ç•™åå•†åŽ†å²
      setMessages(prev => ({
        ...prev,
        solution: prev.solution.map(msg => 
          msg.id === followUpId 
            ? { 
                ...msg, 
                text: llmResult.followUpMessage,
                negotiating: false,
                negotiated: true,
                negotiationHistory: [
                  ...(msg.negotiationHistory || []),
                  {
                    previousText: msg.negotiationHistory?.length > 0 ? msg.text : (msg.originalText || msg.text),
                    negotiationRequest: negotiationText,
                    timestamp: new Date().toISOString()
                  }
                ],
                originalText: msg.originalText || originalFollowUp.text
              }
            : msg
        )
      }))

      // åå•†å®ŒæˆåŽï¼Œå°†åå•†åŽçš„è¿½é—®è‡ªåŠ¨å¡«å…¥è¾“å…¥æ¡†
      if (onSetInput && typeof onSetInput === 'function') {
        onSetInput(llmResult.followUpMessage)
      }

    } catch (error) {
      console.error('åå•†è¿½é—®é”™è¯¯:', error)
      // åå•†å¤±è´¥ï¼Œé€€å‡ºåå•†æ¨¡å¼
      cancelFollowUpNegotiation(followUpId)
    }
  }, [messages.problem, messages.solution, currentScenario, addMessage, cancelFollowUpNegotiation])

  // æ–°å¢žï¼šæ‹’ç»è¿½é—®å¹¶é‡æ–°ç”Ÿæˆ
  const rejectFollowUp = useCallback(async (followUpId) => {
    // æ ‡è®°è¿½é—®ä¸ºå·²æ‹’ç»
    setMessages(prev => ({
      ...prev,
      solution: prev.solution.map(msg => 
        msg.id === followUpId 
          ? { ...msg, feedbackGiven: true, accepted: false }
          : msg
      )
    }))

    // é‡æ–°ç”Ÿæˆè¿½é—®
    await generateFollowUp()
  }, [generateFollowUp])

  // æ–°å¢žï¼šæŽ¥å—æ™ºèƒ½è¿½é—®
  const acceptIntelligentFollowUp = useCallback((followUpId, onSetInput) => {
    const followUpMessage = messages.solution.find(msg => msg.id === followUpId)
    if (!followUpMessage) return

    // æ ‡è®°ä¸ºå·²æŽ¥å—
    setMessages(prev => ({
      ...prev,
      solution: prev.solution.map(msg => 
        msg.id === followUpId 
          ? { ...msg, feedbackGiven: true, accepted: true }
          : msg
      )
    }))

    // å°†è¿½é—®å†…å®¹å¡«å…¥è¾“å…¥æ¡†
    if (onSetInput && typeof onSetInput === 'function') {
      onSetInput(followUpMessage.text)
    }

    // è®¾ä¸ºç›´å‘å€™é€‰ï¼Œç­‰å¾…å¯¹æ¯”ç¡®è®¤
    setDirectSendCandidate({
      type: 'intelligent_followup',
      sourceMessageId: followUpId,
      sourceText: followUpMessage.text,
      createdAt: new Date().toISOString()
    })
  }, [messages.solution])

  // æ–°å¢žï¼šç¡®è®¤ç›´å‘åˆ°é—®é¢˜ç«¯ï¼ˆä¸è½¬è¯‘ï¼‰
  const confirmDirectSendToProblem = useCallback((finalText) => {
    if (!finalText || !finalText.trim()) return

    // ç›´æŽ¥å‘é€åˆ°é—®é¢˜ç«¯
    const directMessage = {
      type: 'ai_response',
      text: finalText.trim(),
      timestamp: new Date().toISOString(),
      isDirectFollowUp: true,
      candidateType: directSendCandidate?.type
    }
    addMessage('problem', directMessage)

    // åœ¨ä¸­ä»‹é¢æ¿è®°å½•ä¸€æ¬¡å¤„ç†è¯´æ˜Ž
    const infoMessage = {
      type: 'processing',
      title: 'è¿½é—®ç›´è¾¾ç”¨æˆ·ç«¯',
      steps: [{
        name: 'å¤„ç†è¯´æ˜Ž',
        content: 'å·²ç¡®è®¤ç›´å‘ï¼Œè·³è¿‡AIè½¬è¯‘'
      }],
      output: finalText.trim(),
      timestamp: new Date().toISOString()
    }
    addMessage('llm', infoMessage)

    // æ¸…ç©ºå€™é€‰
    setDirectSendCandidate(null)
  }, [addMessage, directSendCandidate])

  // New: Cancel direct send process, keep input box content
  const cancelDirectSend = useCallback(() => {
    setDirectSendCandidate(null)
  }, [])

  // æ–°å¢žï¼šå¤–éƒ¨å‡†å¤‡ç›´å‘å€™é€‰ï¼ˆç”¨äºŽâ€œåº”ç”¨å®¢æˆ·å›žå¤â€æŒ‰é’®ï¼‰
  const prepareDirectSendCandidate = useCallback((candidate) => {
    if (!candidate || !candidate.sourceText) return
    setDirectSendCandidate({
      type: candidate.type || 'customer_reply',
      sourceMessageId: candidate.sourceMessageId,
      sourceText: candidate.sourceText,
      createdAt: new Date().toISOString()
    })
  }, [])

  // æ–°å¢žï¼šæ‹’ç»æ™ºèƒ½è¿½é—®
  const rejectIntelligentFollowUp = useCallback(async (followUpId) => {
    // æ ‡è®°è¿½é—®ä¸ºå·²æ‹’ç»
    setMessages(prev => ({
      ...prev,
      solution: prev.solution.map(msg => 
        msg.id === followUpId 
          ? { ...msg, feedbackGiven: true, accepted: false }
          : msg
      )
    }))

    // å¯ä»¥é€‰æ‹©é‡æ–°ç”Ÿæˆæˆ–è€…å›žåˆ°ä¿¡æ¯é€‰æ‹©ç•Œé¢
    setShowMissingInfoPanel(true)
  }, [])

  // æ–°å¢žï¼šåå•†æ™ºèƒ½è¿½é—®
  const negotiateIntelligentFollowUp = useCallback((followUpId) => {
    setMessages(prev => ({
      ...prev,
      solution: prev.solution.map(msg => 
        msg.id === followUpId 
          ? { ...msg, negotiating: true }
          : msg
      )
    }))
  }, [])

  // New: Cancel intelligent follow-up negotiation
  const cancelIntelligentFollowUpNegotiation = useCallback((followUpId) => {
    setMessages(prev => ({
      ...prev,
      solution: prev.solution.map(msg => 
        msg.id === followUpId 
          ? { ...msg, negotiating: false }
          : msg
      )
    }))
  }, [])

  // æ–°å¢žï¼šå‘é€æ™ºèƒ½è¿½é—®åå•†è¯·æ±‚
  const sendIntelligentFollowUpNegotiationRequest = useCallback(async (followUpId, negotiationText, onSetInput) => {
    if (!negotiationText.trim()) return

    try {
      // èŽ·å–åŽŸå§‹è¿½é—®
      const originalFollowUp = messages.solution.find(msg => msg.id === followUpId)
      if (!originalFollowUp) return

      // æž„å»ºåå•†ä¸Šä¸‹æ–‡
      const chatHistory = [
        ...messages.problem
          .filter(msg => msg.type === 'user' || msg.type === 'ai_response')
          .map(msg => ({ ...msg, panel: 'problem' })),
        ...messages.solution
          .filter(msg => msg.type === 'llm_request' || msg.type === 'user' || msg.type === 'ai_response')
          .map(msg => ({ ...msg, panel: 'solution' }))
      ].sort((a, b) => new Date(a.timestamp) - new Date(b.timestamp))

      // ç”Ÿæˆåå•†åŽçš„è¿½é—®
      const llmResult = await processWithLLM({
        type: 'negotiate_followup',
        content: {
          originalFollowUp: originalFollowUp.text || originalFollowUp.output,
          negotiationRequest: negotiationText,
          negotiationHistory: originalFollowUp.negotiationHistory || []
        },
        scenario: currentScenario,
        chatHistory: chatHistory,
        aiChatHistory: aiChatHistory,
        streamCallbacks: streamCallbacks
      })

      // æ·»åŠ LLMå¤„ç†è¿‡ç¨‹åˆ°ä¸­ä»‹é¢æ¿
      const llmMessage = {
        type: 'processing',
        title: 'åå•†ä¿®æ”¹æ™ºèƒ½è¿½é—®',
        steps: llmResult.steps,
        output: llmResult.followUpMessage,
        timestamp: new Date().toISOString()
      }
      addMessage('llm', llmMessage)

      // æ›´æ–°åŽŸè¿½é—®ä¸ºåå•†åŽçš„ç‰ˆæœ¬ï¼Œä¿ç•™åå•†åŽ†å²
      setMessages(prev => ({
        ...prev,
        solution: prev.solution.map(msg => 
          msg.id === followUpId 
            ? { 
                ...msg, 
                text: llmResult.followUpMessage,
                negotiating: false,
                negotiated: true,
                negotiationHistory: [
                  ...(msg.negotiationHistory || []),
                  {
                    previousText: msg.negotiationHistory?.length > 0 ? msg.text : (msg.originalText || msg.text),
                    negotiationRequest: negotiationText,
                    timestamp: new Date().toISOString()
                  }
                ],
                originalText: msg.originalText || originalFollowUp.text
              }
            : msg
        )
      }))

      // åå•†å®ŒæˆåŽï¼Œå°†åå•†åŽçš„è¿½é—®è‡ªåŠ¨å¡«å…¥è¾“å…¥æ¡†
      if (onSetInput && typeof onSetInput === 'function') {
        onSetInput(llmResult.followUpMessage)
      }

    } catch (error) {
      console.error('åå•†æ™ºèƒ½è¿½é—®é”™è¯¯:', error)
      // åå•†å¤±è´¥ï¼Œé€€å‡ºåå•†æ¨¡å¼
      cancelIntelligentFollowUpNegotiation(followUpId)
    }
  }, [messages.problem, messages.solution, currentScenario, addMessage, cancelIntelligentFollowUpNegotiation])

  // ç”Ÿæˆéƒ¨é—¨è”ç»œæŒ‡ä»¤
  // AIåä½œå¯¹è¯åŠŸèƒ½ - å…·æœ‰å®Œæ•´ä¸Šä¸‹æ–‡è®°å¿†
  const chatWithAI = useCallback(async (question) => {
    console.log('ðŸ¤– å¼€å§‹AIåä½œå¯¹è¯:', { question, previousChatHistory: aiChatHistory.length })

    try {
      // First add user question to AI conversation history
      const userMessage = {
        role: 'user',
        content: question,
        timestamp: new Date().toISOString()
      }
      
      // å°†ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ°UIæ˜¾ç¤ºçš„æ¶ˆæ¯åˆ—è¡¨ä¸­
      const userDisplayMessage = {
        type: 'user',
        text: question,
        timestamp: new Date().toISOString(),
        generatedInMode: chatMode // Save mode when generated
      }
      addMessage('solution', userDisplayMessage)
      
      // æž„å»ºå®Œæ•´çš„AIåä½œå¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆåŒ…æ‹¬ä¹‹å‰çš„æ‰€æœ‰å¯¹è¯ï¼‰
      const fullAiChatHistory = [...aiChatHistory, userMessage]
      
      // æž„å»ºå®Œæ•´çš„èŠå¤©ä¸Šä¸‹æ–‡ - åŒ…å«æ‰€æœ‰ç›¸å…³ä¿¡æ¯
      const fullChatContext = {
        // 1. AIåä½œå¯¹è¯åŽ†å² (ChatGPTé£Žæ ¼çš„å¯¹è¯è®°å½•)
        aiChatHistory: fullAiChatHistory.map(msg => ({
          role: msg.role,
          content: msg.content,
          timestamp: msg.timestamp
        })),
        
        // 2. å®¢æˆ·é—®é¢˜ç«¯çš„å®Œæ•´å¯¹è¯åŽ†å² - åŒ…å«æ‰€æœ‰æ¶ˆæ¯
        customerMessages: messages.problem.map(msg => ({
          type: msg.type,
          content: msg.text || msg.content || '',
          timestamp: msg.timestamp,
          role: msg.type === 'user' ? 'customer' : 'system',
          panel: 'problem'
        })),
        
        // 3. è§£å†³æ–¹æ¡ˆç«¯çš„æ‰€æœ‰æ¶ˆæ¯ - ä¸è¿‡æ»¤ï¼ŒåŒ…å«å…¨éƒ¨
        solutionMessages: messages.solution.map(msg => ({
          type: msg.type,
          content: msg.text || msg.content || '',
          timestamp: msg.timestamp,
          panel: 'solution'
        })),
        
        // 4. LLMå¤„ç†ç«¯çš„æ‰€æœ‰æ¶ˆæ¯
        llmMessages: messages.llm ? messages.llm.map(msg => ({
          type: msg.type,
          content: msg.output || msg.text || msg.content || '',
          timestamp: msg.timestamp,
          panel: 'llm'
        })) : [],
        
        // 5. å½“å‰åœºæ™¯ä¿¡æ¯
        scenario: {
          name: currentScenario?.name || 'retail',
          problemRole: currentScenario?.problemRole || 'å®¢æˆ·',
          solutionRole: currentScenario?.solutionRole || 'å®¢æœä»£è¡¨'
        },
        
        // 6. å½“å‰èŠå¤©æ¨¡å¼
        chatMode
      }
      
      console.log('ðŸ“‹ å®Œæ•´å¯¹è¯ä¸Šä¸‹æ–‡:', {
        aiChatHistoryLength: fullChatContext.aiChatHistory.length,
        customerMessagesLength: fullChatContext.customerMessages.length,
        solutionMessagesLength: fullChatContext.solutionMessages.length,
        llmMessagesLength: fullChatContext.llmMessages.length,
        scenario: fullChatContext.scenario
      })
      
      // è®¾ç½®æµå¼ä¸Šä¸‹æ–‡ï¼ŒæŒ‡å®šæ¶ˆæ¯ç±»åž‹
      const streamContext = {
        ...fullChatContext,
        messageType: 'ai_chat',
        question: question
      }

      // è°ƒç”¨LLMè¿›è¡ŒAIå¯¹è¯ï¼Œä¼ é€’å®Œæ•´çš„å¯¹è¯åŽ†å²å’Œä¸Šä¸‹æ–‡
      const llmResult = await processWithLLM({
        type: 'ai_chat',
        content: question,
        scenario: currentScenario,
        // ä¼ é€’å®Œæ•´çš„èŠå¤©ä¸Šä¸‹æ–‡
        chatHistory: fullChatContext,
        // ä¿æŒå‘åŽå…¼å®¹
        aiChatHistory: fullAiChatHistory,
        currentContext: fullChatContext,
        streamCallbacks: {
          ...streamCallbacks,
          onStreamStart: (controller, context) => {
            // è°ƒç”¨åŽŸå§‹å›žè°ƒ
            streamCallbacks.onStreamStart(controller, {
              ...context,
              messageType: 'ai_chat',
              question: question,
              chatMode: chatMode // ä¼ é€’å½“å‰èŠå¤©æ¨¡å¼
            })
          }
        }
      })

      // åˆ›å»ºAIå›žå¤æ¶ˆæ¯
      const assistantMessage = {
        role: 'assistant',
        content: llmResult.answer,
        timestamp: new Date().toISOString()
      }

      // æ›´æ–°AIå¯¹è¯åŽ†å²ï¼ˆæ·»åŠ ç”¨æˆ·é—®é¢˜å’ŒAIå›žå¤ï¼‰
      setAiChatHistory(prev => [...prev, userMessage, assistantMessage])

      // æµå¼æ¶ˆæ¯å·²ç»é€šè¿‡streamCallbacksæ·»åŠ åˆ°solutioné¢æ¿äº†ï¼Œè¿™é‡Œéœ€è¦æ›´æ–°ä¸ºå®Œæ•´çš„ai_chatæ ¼å¼
      setMessages(prev => ({
        ...prev,
        solution: prev.solution.map(msg => 
          msg.isStreaming && msg.streamId === streamingMessage?.streamId 
            ? { 
                ...msg, 
                type: 'ai_chat',
                question: question,
                answer: llmResult.answer,
                text: llmResult.answer,
                error: llmResult.error,
                conversationId: Date.now(),
                isStreaming: false
              }
            : msg
        )
      }))

      console.log('âœ… AIåä½œå¯¹è¯å®Œæˆ:', { 
        answer: llmResult.answer, 
        chatHistoryLength: fullAiChatHistory.length + 1 
      })

    } catch (error) {
      console.error('âŒ AIåä½œå¯¹è¯å¤±è´¥:', error)
      
      // æ›´æ–°æ­£åœ¨æµå¼æ˜¾ç¤ºçš„æ¶ˆæ¯ä¸ºé”™è¯¯çŠ¶æ€
      if (streamingMessage) {
        setMessages(prev => ({
          ...prev,
          solution: prev.solution.map(msg => 
            msg.streamId === streamingMessage.streamId 
              ? { 
                  ...msg,
                  type: 'ai_chat',
                  question: question,
                  answer: 'AIåä½œåŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨ï¼Œå»ºè®®ç¨åŽå†æ¬¡å°è¯•ã€‚æˆ‘å·²è®°ä½æœ¬æ¬¡å¯¹è¯ï¼Œæ‚¨å¯ä»¥ç¨åŽç»§ç»­',
                  text: 'AIåä½œåŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨ï¼Œå»ºè®®ç¨åŽå†æ¬¡å°è¯•ã€‚æˆ‘å·²è®°ä½æœ¬æ¬¡å¯¹è¯ï¼Œæ‚¨å¯ä»¥ç¨åŽç»§ç»­',
                  error: true,
                  isStreaming: false
                }
              : msg
          )
        }))
      }
      
      // å³ä½¿å‡ºé”™ï¼Œä¹Ÿè¦è®°å½•ç”¨æˆ·çš„é—®é¢˜åˆ°åŽ†å²ä¸­
      const userMessage = {
        role: 'user', 
        content: question,
        timestamp: new Date().toISOString()
      }
      setAiChatHistory(prev => [...prev, userMessage])
    }
  }, [addMessage, currentScenario, aiChatHistory, messages.problem, chatMode, streamCallbacks])

  // æ¸…ç©ºAIåä½œå¯¹è¯åŽ†å²
  const clearAiChatHistory = useCallback(() => {
    setAiChatHistory([])
    console.log('ðŸ—‘ï¸ AIåä½œå¯¹è¯åŽ†å²å·²æ¸…ç©º')
  }, [])

  // æ™ºèƒ½ç”Ÿæˆç»™å®¢æˆ·çš„å›žå¤ï¼ˆåŸºäºŽä»»æ„ä¸€æ¡AIæ¶ˆæ¯å†…å®¹ï¼‰
  const generateCustomerReplyForMessage = useCallback(async (message) => {
    console.log('ðŸ” useMessageFlow - generateCustomerReplyForMessage è¢«è°ƒç”¨')
    console.log('ðŸ” æŽ¥æ”¶åˆ°çš„æ¶ˆæ¯å‚æ•°:', message)
    try {
      // é€‰æ‹©æœ€åˆé€‚çš„åŸºç¡€å†…å®¹
      const baseContent = (
        message?.translation ||
        message?.suggestion ||
        message?.aiAdvice ||
        message?.answer ||
        message?.text ||
        ''
      ).toString()

      console.log('ðŸ” å®¢æˆ·å›žå¤ç”Ÿæˆè°ƒè¯•:', { 
        message, 
        baseContent: baseContent.substring(0, 100) + '...',
        isEmpty: !baseContent.trim()
      })

      if (!baseContent.trim()) {
        console.log('âŒ åŸºç¡€å†…å®¹ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆå®¢æˆ·å›žå¤')
        return ''
      }

      // åœ¨åä½œå·¥ä½œå°æ·»åŠ å ä½æ°”æ³¡
      const candidateId = `candidate_${Date.now()}`
      const placeholder = {
        id: candidateId,
        type: 'customer_reply_candidate',
        text: 'Generating reply...',
        timestamp: new Date().toISOString(),
        isStreaming: true
      }
      setMessages(prev => ({
        ...prev,
        solution: [...prev.solution, placeholder]
      }))

      // æž„å»ºå®Œæ•´çš„èŠå¤©åŽ†å² - ä¸Žç¡®è®¤å‘é€æµç¨‹ä¿æŒä¸€è‡´
      const chatHistory = [
        ...messages.problem
          .filter(msg => msg.type === 'user' || msg.type === 'ai_response')
          .map(msg => ({ ...msg, panel: 'problem' })),
        ...messages.solution
          .filter(msg => msg.type === 'llm_request' || msg.type === 'user' || msg.type === 'ai_response')
          .map(msg => ({ ...msg, panel: 'solution' }))
      ].sort((a, b) => new Date(a.timestamp) - new Date(b.timestamp))

      // åˆ›å»ºåªæ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹ä½†ä¸åˆ›å»ºæ¶ˆæ¯çš„ç‰¹æ®Šæµå¼å›žè°ƒï¼ˆä¿®æ­£ï¼šå¯¹é½åŽŸæœ‰å›žè°ƒç­¾åï¼‰
      const customerReplyStreamCallbacks = {
        ...streamCallbacks,
        onStreamStart: (controller, context) => {
          console.log('ðŸš€ å®¢æˆ·å›žå¤ç”Ÿæˆ: æµå¼å¼€å§‹ï¼Œåªæ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹')
          const customContext = {
            ...context,
            isCustomerReply: true,
            suppressMessageCreation: true
          }
          if (streamCallbacks && streamCallbacks.onStreamStart) {
            streamCallbacks.onStreamStart(controller, customContext)
          }
        },
        onThinking: (fragment, fullContent) => {
          if (streamCallbacks && streamCallbacks.onThinking) {
            streamCallbacks.onThinking(fragment, fullContent)
          }
        },
        onAnswering: (fragment, fullContent) => {
          if (streamCallbacks && streamCallbacks.onAnswering) {
            streamCallbacks.onAnswering(fragment, fullContent)
          }
        },
        onStreamEnd: (finalThinking, finalAnswer) => {
          console.log('âœ… å®¢æˆ·å›žå¤ç”Ÿæˆ: æµå¼ç»“æŸ')
          if (streamCallbacks && streamCallbacks.onStreamEnd) {
            streamCallbacks.onStreamEnd(finalThinking, finalAnswer)
          }
        },
        onStreamError: (error) => {
          console.error('âŒ å®¢æˆ·å›žå¤ç”Ÿæˆ: æµå¼é”™è¯¯', error)
          if (streamCallbacks && streamCallbacks.onStreamError) {
            streamCallbacks.onStreamError(error)
          }
        }
      }

      const llmResult = await processWithLLM({
        type: 'solution_response',
        content: baseContent,
        context: 'solution_to_problem',
        scenario: currentScenario,
        chatHistory: chatHistory,
        aiChatHistory: aiChatHistory,
        streamCallbacks: customerReplyStreamCallbacks // ä½¿ç”¨ç‰¹æ®Šçš„æµå¼å›žè°ƒï¼Œåªæ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹
      })

      console.log('ðŸ” LLMç»“æžœ:', { llmResult, optimizedMessage: llmResult?.optimizedMessage })

      // ä¸åšä»»ä½•å¤„ç†ã€ç­›é€‰ã€æˆªæ–­ï¼Œç›´æŽ¥ä½¿ç”¨ç”Ÿæˆçš„å†…å®¹
      const optimized = llmResult?.optimizedMessage || ''
      
      console.log('ðŸ” æœ€ç»ˆä¼˜åŒ–ç»“æžœï¼ˆæ— å¤„ç†ï¼‰:', { optimized, isEmpty: !optimized })

      // æ›´æ–°å ä½æ°”æ³¡ä¸ºç”Ÿæˆç»“æžœ
      setMessages(prev => ({
        ...prev,
        solution: prev.solution.map(msg =>
          msg.id === candidateId
            ? { ...msg, text: optimized || 'ï¼ˆç©ºï¼‰', isStreaming: false }
            : msg
        )
      }))

      return optimized
    } catch (error) {
      console.error('ç”Ÿæˆå®¢æˆ·å›žå¤å¤±è´¥:', error)
      console.log('ðŸ” é”™è¯¯è¯¦æƒ…:', { error: error.message, stack: error.stack })
      // å¤±è´¥æ—¶æ ‡è®°å ä½ä¸ºé”™è¯¯
      setMessages(prev => ({
        ...prev,
        solution: prev.solution.map(msg =>
          msg.type === 'customer_reply_candidate' && msg.isStreaming
            ? { ...msg, text: 'ç”Ÿæˆå¤±è´¥ï¼Œè¯·é‡è¯•', isStreaming: false, error: true }
            : msg
        )
      }))
      return ''
    }
  }, [messages.problem, messages.solution, currentScenario])

  // New: Negotiate modifications to customer reply draft
  const negotiateCustomerReplyCandidate = useCallback(async (candidateId, negotiationText, onUpdateContent) => {
    if (!negotiationText || !negotiationText.trim()) return

    try {
      // Find original draft
      const originalCandidate = messages.solution.find(msg => msg.type === 'customer_reply_candidate' && msg.id === candidateId)
      if (!originalCandidate) {
        console.error('Customer reply draft not found, candidateId:', candidateId)
        return
      }

      // æž„å»ºå®Œæ•´çš„èŠå¤©åŽ†å²ï¼ˆä¸Žç”Ÿæˆä¸€è‡´ï¼‰
      const chatHistory = [
        ...messages.problem
          .filter(msg => msg.type === 'user' || msg.type === 'ai_response')
          .map(msg => ({ ...msg, panel: 'problem' })),
        ...messages.solution
          .filter(msg => msg.type === 'llm_request' || msg.type === 'user' || msg.type === 'ai_response')
          .map(msg => ({ ...msg, panel: 'solution' }))
      ].sort((a, b) => new Date(a.timestamp) - new Date(b.timestamp))

      // Combine negotiation request text: provide modification requirements based on original draft
      const negotiationBase = `ã€Original Customer Reply Draftã€‘\n${originalCandidate.text}\n\nã€Modification Requirementsã€‘\n${negotiationText}\n\nPlease output the optimized customer reply directly based on the modification requirements, keeping it concise and friendly.`

      const llmResult = await processWithLLM({
        type: 'solution_response',
        content: negotiationBase,
        context: 'solution_to_problem',
        scenario: currentScenario,
        chatHistory: chatHistory,
        aiChatHistory: aiChatHistory,
        streamCallbacks: null
      })

      const optimized = llmResult?.optimizedMessage || ''

      // Update original draft text
      setMessages(prev => ({
        ...prev,
        solution: prev.solution.map(msg =>
          msg.id === candidateId && msg.type === 'customer_reply_candidate'
            ? { ...msg, text: optimized || msg.text, negotiated: true }
            : msg
        )
      }))

      if (onUpdateContent && typeof onUpdateContent === 'function') {
        onUpdateContent(optimized)
      }
    } catch (error) {
      console.error('åå•†ä¿®æ”¹å®¢æˆ·å›žå¤å¤±è´¥:', error)
    }
  }, [messages.problem, messages.solution, currentScenario])

  // ç”Ÿæˆéƒ¨é—¨è”ç»œæŒ‡ä»¤ï¼ˆä»…è”ç»œæŒ‡ä»¤ï¼‰
  const generateDepartmentContactOnly = useCallback(async () => {
    if (iterationProcessing) return

    setIterationProcessing(true)

    try {
      // èŽ·å–æœ€æ–°çš„å¯¹è¯å†…å®¹ä½œä¸ºè”ç»œæŒ‡ä»¤çš„åŸºç¡€
      const recentMessages = [
        ...messages.problem.filter(m => m.type === 'user' || m.type === 'ai_response').slice(-2),
        ...messages.solution.filter(m => m.type === 'user' || m.type === 'ai_response').slice(-2)
      ].sort((a, b) => new Date(a.timestamp) - new Date(b.timestamp))

      const currentContent = recentMessages.map(msg => msg.text).join('\n') || 'åŸºäºŽå½“å‰å¯¹è¯ç”Ÿæˆè”ç»œæŒ‡ä»¤'

      // æž„å»ºå®Œæ•´çš„èŠå¤©åŽ†å²
      const chatHistory = [
        // é—®é¢˜ç«¯çš„æ‰€æœ‰æ¶ˆæ¯ï¼šç”¨æˆ·è¾“å…¥ + AIä¼˜åŒ–åŽçš„å›žå¤
        ...messages.problem
          .filter(msg => msg.type === 'user' || msg.type === 'ai_response')
          .map(msg => ({ ...msg, panel: 'problem' })),
        // æ–¹æ¡ˆç«¯çš„æ‰€æœ‰æ¶ˆæ¯ï¼šAIè½¬è¯‘çš„è¯·æ±‚ + ä¼ä¸šç”¨æˆ·è¾“å…¥ + AIå›žå¤
        ...messages.solution
          .filter(msg => msg.type === 'llm_request' || msg.type === 'user' || msg.type === 'ai_response')
          .map(msg => ({ ...msg, panel: 'solution' }))
      ].sort((a, b) => new Date(a.timestamp) - new Date(b.timestamp))

      // ç”Ÿæˆéƒ¨é—¨è”ç»œæŒ‡ä»¤
      const llmResult = await processWithLLM({
        type: 'generate_department_contact_only',
        content: currentContent,
        scenario: currentScenario,
        chatHistory: chatHistory,
        aiChatHistory: aiChatHistory,
        streamCallbacks: streamCallbacks
      })

      // æ·»åŠ LLMå¤„ç†è¿‡ç¨‹åˆ°ä¸­ä»‹é¢æ¿
      const llmMessage = {
        type: 'processing',
        title: 'ç”Ÿæˆéƒ¨é—¨è”ç»œ',
        steps: llmResult.steps,
        structuredOutput: llmResult.structuredOutput,
        timestamp: new Date().toISOString()
      }
      addMessage('llm', llmMessage)

      console.log('âœ… éƒ¨é—¨è”ç»œæŒ‡ä»¤ç”Ÿæˆå®Œæˆ')

    } catch (error) {
      console.error('ç”Ÿæˆéƒ¨é—¨è”ç»œæŒ‡ä»¤æ—¶å‡ºé”™:', error)
      // æ·»åŠ é”™è¯¯æ¶ˆæ¯
      const errorMessage = {
        type: 'processing',
        title: 'ç”Ÿæˆéƒ¨é—¨è”ç»œå‡ºé”™',
        steps: [{
          name: 'é”™è¯¯ä¿¡æ¯',
          content: 'æŠ±æ­‰ï¼Œç”Ÿæˆéƒ¨é—¨è”ç»œæŒ‡ä»¤æ—¶å‡ºçŽ°äº†é”™è¯¯ï¼Œè¯·ç¨åŽé‡è¯•ã€‚'
        }],
        timestamp: new Date().toISOString()
      }
      addMessage('llm', errorMessage)
    } finally {
      setIterationProcessing(false)
    }
  }, [addMessage, currentScenario, messages.problem, messages.solution, iterationProcessing])

  const generateDepartmentContact = useCallback(async (suggestion) => {
    if (iterationProcessing) return

    setIterationProcessing(true)

    try {
      // æž„å»ºå®Œæ•´çš„èŠå¤©åŽ†å²
      const chatHistory = [
        // é—®é¢˜ç«¯çš„æ‰€æœ‰æ¶ˆæ¯ï¼šç”¨æˆ·è¾“å…¥ + AIä¼˜åŒ–åŽçš„å›žå¤
        ...messages.problem
          .filter(msg => msg.type === 'user' || msg.type === 'ai_response')
          .map(msg => ({ ...msg, panel: 'problem' })),
        // æ–¹æ¡ˆç«¯çš„æ‰€æœ‰æ¶ˆæ¯ï¼šAIè½¬è¯‘çš„è¯·æ±‚ + ä¼ä¸šç”¨æˆ·è¾“å…¥ + AIå›žå¤
        ...messages.solution
          .filter(msg => msg.type === 'llm_request' || msg.type === 'user' || msg.type === 'ai_response')
          .map(msg => ({ ...msg, panel: 'solution' }))
      ].sort((a, b) => new Date(a.timestamp) - new Date(b.timestamp))

      // ç”Ÿæˆéƒ¨é—¨è”ç»œæŒ‡ä»¤
      const llmResult = await processWithLLM({
        type: 'generate_department_contact',
        content: suggestion,
        scenario: currentScenario,
        chatHistory: chatHistory,
        aiChatHistory: aiChatHistory,
        streamCallbacks: streamCallbacks
      })

      // æ·»åŠ LLMå¤„ç†è¿‡ç¨‹åˆ°ä¸­ä»‹é¢æ¿
      const llmMessage = {
        type: 'processing',
        title: 'ç”Ÿæˆå®¢æˆ·å›žå¤å’Œéƒ¨é—¨è”ç»œ',
        steps: llmResult.steps,
        output: `å®¢æˆ·å›žå¤ï¼š${llmResult.customerReply}\n\nè”ç»œæŒ‡ä»¤ï¼š${llmResult.contactInstruction}`,
        structuredOutput: llmResult.structuredOutput,
        timestamp: new Date().toISOString()
      }
      addMessage('llm', llmMessage)

      // å°†è”ç»œæŒ‡ä»¤æ·»åŠ åˆ°æ–¹æ¡ˆç«¯ï¼ˆä½œä¸ºç‰¹æ®Šæ¶ˆæ¯ç±»åž‹ï¼‰
      const contactMessage = {
        type: 'department_contact',
        customerReply: llmResult.customerReply,
        contactInstruction: llmResult.contactInstruction,
        timestamp: new Date().toISOString(),
        id: `contact_${Date.now()}`,
        instructionSent: false, // åˆå§‹åŒ–ä¸ºæœªå‘é€çŠ¶æ€
        sentTimestamp: null,
        customerReplyApplied: false, // åˆå§‹åŒ–ä¸ºæœªåº”ç”¨çŠ¶æ€
        appliedTimestamp: null
      }
      addMessage('solution', contactMessage)

    } catch (error) {
      console.error('ç”Ÿæˆéƒ¨é—¨è”ç»œæŒ‡ä»¤æ—¶å‡ºé”™:', error)
      // æ·»åŠ é”™è¯¯æ¶ˆæ¯
      const errorMessage = {
        type: 'processing',
        title: 'ç”Ÿæˆå®¢æˆ·å›žå¤å’Œéƒ¨é—¨è”ç»œå‡ºé”™',
        steps: [{
          name: 'é”™è¯¯ä¿¡æ¯',
          content: 'æŠ±æ­‰ï¼Œç”Ÿæˆéƒ¨é—¨è”ç»œæŒ‡ä»¤æ—¶å‡ºçŽ°äº†é”™è¯¯ï¼Œè¯·ç¨åŽé‡è¯•ã€‚'
        }],
        timestamp: new Date().toISOString()
      }
      addMessage('llm', errorMessage)
    } finally {
      setIterationProcessing(false)
    }
  }, [addMessage, currentScenario, messages.problem, messages.solution, iterationProcessing])

  // æ ‡è®°è”ç»œæŒ‡ä»¤ä¸ºå·²å‘é€
  const markContactInstructionSent = useCallback((contactId) => {
    setMessages(prev => ({
      ...prev,
      solution: prev.solution.map(msg => 
        msg.id === contactId && msg.type === 'department_contact' ? {
          ...msg,
          instructionSent: true,
          sentTimestamp: new Date().toISOString()
        } : msg
      )
    }))
  }, [])

  // æ ‡è®°å®¢æˆ·å›žå¤ä¸ºå·²åº”ç”¨
  const markCustomerReplyApplied = useCallback((contactId) => {
    setMessages(prev => ({
      ...prev,
      solution: prev.solution.map(msg => 
        msg.id === contactId && msg.type === 'department_contact' ? {
          ...msg,
          customerReplyApplied: true,
          appliedTimestamp: new Date().toISOString()
        } : msg
      )
    }))
    console.log('âœ… å®¢æˆ·å›žå¤å·²æ ‡è®°ä¸ºåº”ç”¨çŠ¶æ€', contactId)
  }, [])

  // æ–°å¢žï¼šæ¸…ç©ºæ‰€æœ‰çŠ¶æ€
  const clearAllStates = useCallback(() => {
    setMessages({
      problem: [],
      llm: [],
      solution: []
    })
    setIterationMode(false)
    setPendingResponse(null)
    setMissingInfoOptions([])
    setShowMissingInfoPanel(false)
    setCurrentNeedsAnalysis(null)
    setChatMode('normal')
    setDepartmentModalVisible(false)
    setEmergencyModalVisible(false)
  }, [])

  // æ–°å¢žï¼šç”Ÿæˆéƒ¨é—¨è”ç»œå»ºè®®ï¼ˆæ”¹è¿›ç‰ˆï¼šç¡®ä¿AIå‚ä¸Žï¼‰
  const generateDepartmentContactSuggestion = useCallback(async () => {
    console.log('ðŸ“ž generateDepartmentContactSuggestionè¢«è°ƒç”¨')
    console.log('ðŸ“Š å½“å‰çŠ¶æ€:', { 
      llmProcessing, 
      iterationProcessing, 
      problemMessages: messages.problem.length,
      solutionMessages: messages.solution.length,
      currentScenario 
    })
    
    if (llmProcessing || iterationProcessing) {
      console.log('â¸ï¸ Skipping execution: processing in progress')
      return
    }

    setIterationProcessing(true)

    try {
      // èŽ·å–æœ€è¿‘çš„å¯¹è¯å†…å®¹
      const recentMessages = [
        ...messages.problem.slice(-3),
        ...messages.solution.slice(-3)
      ].sort((a, b) => new Date(a.timestamp) - new Date(b.timestamp))

      const content = recentMessages
        .map(msg => msg.text || msg.output || '')
        .filter(text => text.trim())
        .join('\n')

      console.log('ðŸ“ å¯¹è¯å†…å®¹:', { content: content.substring(0, 100), isEmpty: !content.trim() })

      // Improvement: Call AI to generate contact suggestions and internal instructions regardless of conversation content
      const contextInfo = content.trim() ? content : 'æš‚æ— å…·ä½“å¯¹è¯å†…å®¹ï¼Œéœ€è¦ç”Ÿæˆé€šç”¨çš„éƒ¨é—¨è”ç»œæ–¹æ¡ˆ'
      
      console.log('ðŸ¤– è°ƒç”¨LLMç”Ÿæˆæ™ºèƒ½éƒ¨é—¨è”ç»œå»ºè®®å’Œå†…éƒ¨æŒ‡ä»¤ï¼ˆæµå¼æ˜¾ç¤ºï¼‰')
      
      // è®¾ç½®æµå¼ä¸Šä¸‹æ–‡ï¼ŒæŒ‡å®šæ¶ˆæ¯ç±»åž‹ä¸ºéƒ¨é—¨è”ç»œ
      const streamContext = {
        messageType: 'department_contact_with_instructions',
        chatMode: 'department'
      }
      
      // è°ƒç”¨LLMç”Ÿæˆéƒ¨é—¨è”ç»œå»ºè®®ï¼ˆæ–°çš„ç±»åž‹ï¼šåŒ…å«å†…éƒ¨æŒ‡ä»¤ï¼Œæ”¯æŒæµå¼ï¼‰
      await processWithLLM({
        type: 'generate_department_contact_with_instructions',
        content: contextInfo,
        scenario: currentScenario,
        chatHistory: recentMessages,
        streamCallbacks: {
          ...streamCallbacks,
          onStreamStart: (controller, context) => {
            // è°ƒç”¨åŽŸå§‹å›žè°ƒ
            streamCallbacks.onStreamStart(controller, {
              ...context,
              messageType: 'department_contact_with_instructions',
              chatMode: 'department'
            })
          }
        }
      })

      console.log('âœ… éƒ¨é—¨è”ç»œå»ºè®®æµå¼ç”Ÿæˆå®Œæˆ')

    } catch (error) {
      console.error('ç”Ÿæˆéƒ¨é—¨è”ç»œå»ºè®®å¤±è´¥:', error)
      // æä¾›é”™è¯¯åŽçš„é»˜è®¤å»ºè®®
      const errorSuggestion = `âš ï¸ **ç³»ç»Ÿæç¤ºï¼š** ç”Ÿæˆå»ºè®®æ—¶å‡ºçŽ°é”™è¯¯ï¼Œè¯·æ‰‹åŠ¨è”ç»œç›¸å…³éƒ¨é—¨æˆ–åˆ‡æ¢è‡³æ™®é€šå¯¹è¯æ¨¡å¼ã€‚
      
**å¤‡ç”¨è”ç»œæ–¹æ¡ˆï¼š**
ðŸ“ž **å®¢æœä¸»ç®¡** - ç«‹å³è”ç³»å¤„ç†å½“å‰æƒ…å†µ
ðŸ“§ **å†…éƒ¨æŒ‡ä»¤ï¼š** è¯·å°†æ­¤å¯¹è¯è®°å½•å‘é€ç»™ç›¸å…³è´Ÿè´£äºº`
      const suggestionMessage = {
        type: 'department_contact_with_instructions',
        text: errorSuggestion,
        timestamp: new Date().toISOString(),
        isError: true,
        generatedInMode: chatMode // Save mode when generated
      }
      addMessage('solution', suggestionMessage)
    } finally {
      setIterationProcessing(false)
      console.log('ðŸ generateDepartmentContactSuggestionæ‰§è¡Œå®Œæˆ')
    }
  }, [llmProcessing, iterationProcessing, messages.problem, messages.solution, currentScenario, addMessage])

  // æ–°å¢žï¼šèŠå¤©æ¨¡å¼ç®¡ç†æ–¹æ³•
  const switchChatMode = useCallback((mode) => {
    console.log('ðŸ”„ switchChatModeè¢«è°ƒç”¨:', mode)
    setChatMode(mode)
    
    if (mode === 'department') {
      // éƒ¨é—¨è”ç»œæ¨¡å¼ä¸éœ€è¦æ¨¡æ€æ¡†ï¼Œç›´æŽ¥ç”Ÿæˆå»ºè®®
      console.log('ðŸ“ž å¼€å§‹ç”Ÿæˆéƒ¨é—¨è”ç»œå»ºè®®')
      // ç«‹å³æ‰§è¡Œéƒ¨é—¨è”ç»œå»ºè®®ç”Ÿæˆ
      setTimeout(() => {
        generateDepartmentContactSuggestion()
      }, 100) // å°å»¶è¿Ÿç¡®ä¿çŠ¶æ€æ›´æ–°å®Œæˆ
    } else if (mode === 'emergency') {
      setEmergencyModalVisible(true)
    } else {
      // æ™®é€šæ¨¡å¼ï¼Œæ¸…é™¤æ‰€æœ‰æ¨¡æ€æ¡†çŠ¶æ€
      setDepartmentModalVisible(false)
      setEmergencyModalVisible(false)
    }
  }, [generateDepartmentContactSuggestion])

  // æ–°å¢žï¼šå¤„ç†ç´§æ€¥æƒ…å†µ
  const handleEmergencyMode = useCallback(async (urgencyLevel, description) => {
    if (llmProcessing || iterationProcessing) return

    setIterationProcessing(true)

    try {
      // æž„å»ºç´§æ€¥å¤„ç†è¯·æ±‚
      const emergencyData = {
        urgencyLevel, // 'high', 'critical', 'urgent'
        description,
        context: {
          scenario: currentScenario,
          recentMessages: [
            ...messages.problem.slice(-2),
            ...messages.solution.slice(-2)
          ]
        },
        timestamp: new Date().toISOString()
      }

      // è®¾ç½®æµå¼ä¸Šä¸‹æ–‡ï¼ŒæŒ‡å®šæ¶ˆæ¯ç±»åž‹ä¸ºç´§æ€¥å“åº”
      const emergencyStreamCallbacks = {
        ...streamCallbacks,
        onStreamStart: (controller, context) => {
          if (streamCallbacks && streamCallbacks.onStreamStart) {
            streamCallbacks.onStreamStart(controller, {
              ...context,
              messageType: 'emergency_response',
              chatMode: 'emergency',
              urgencyLevel: urgencyLevel,
              description: description,
              escalated: urgencyLevel === 'critical'
            })
          }
        }
      }

      // è°ƒç”¨LLMç”Ÿæˆç´§æ€¥å¤„ç†æ–¹æ¡ˆï¼ˆæµå¼å›žè°ƒä¼šè‡ªåŠ¨åˆ›å»ºæ¶ˆæ¯ï¼Œæ— éœ€æ‰‹åŠ¨addMessageï¼‰
      const llmResult = await processWithLLM({
        type: 'emergency_handling',
        content: emergencyData,
        scenario: currentScenario,
        streamCallbacks: emergencyStreamCallbacks
      })

      // å¦‚æžœæ˜¯å…³é”®çº§åˆ«ï¼Œè‡ªåŠ¨å‘é€å‡çº§é€šçŸ¥
      if (urgencyLevel === 'critical') {
        const escalationMessage = {
          type: 'escalation_notice',
          text: `å…³é”®çº§åˆ«ç´§æ€¥æƒ…å†µå·²è‡ªåŠ¨ä¸ŠæŠ¥ç®¡ç†å±‚\næ—¶é—´ï¼š${new Date().toLocaleString()}\næè¿°ï¼š${description}`,
          timestamp: new Date().toISOString()
        }
        addMessage('solution', escalationMessage)
      }

    } catch (error) {
      console.error('å¤„ç†ç´§æ€¥æƒ…å†µå¤±è´¥:', error)
      const errorMessage = {
        type: 'emergency_error',
        text: 'ç´§æ€¥å¤„ç†ç³»ç»Ÿæš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç«‹å³è”ç³»ç›´å±žä¸Šçº§æˆ–æ‹¨æ‰“ç´§æ€¥è”ç³»ç”µè¯ã€‚',
        timestamp: new Date().toISOString(),
        isError: true
      }
      addMessage('solution', errorMessage)
    } finally {
      setIterationProcessing(false)
    }
  }, [llmProcessing, iterationProcessing, currentScenario, messages.problem, messages.solution, addMessage])

  // æ–°å¢žï¼šå…³é—­æ¨¡å¼å¼¹çª—
  const closeModeModal = useCallback((mode) => {
    if (mode === 'department') {
      setDepartmentModalVisible(false)
    } else if (mode === 'emergency') {
      setEmergencyModalVisible(false)
    }
    setChatMode('normal')
  }, [])

  // æ–°å¢žï¼šåªå…³é—­ç´§æ€¥æ¨¡æ€æ¡†ï¼Œä¿æŒç´§æ€¥çŠ¶æ€
  const closeEmergencyModalOnly = useCallback(() => {
    setEmergencyModalVisible(false)
    // ä¸æ”¹å˜èŠå¤©æ¨¡å¼ï¼Œä¿æŒemergencyçŠ¶æ€
  }, [])

  return {
    messages,
    llmProcessing,
    llmProcessingContext,
    iterationProcessing,
    iterationMode,
    pendingResponse,
    directSendCandidate,
    // æ–°å¢žçš„çŠ¶æ€å’Œæ–¹æ³•
    missingInfoOptions,
    showMissingInfoPanel,
    currentNeedsAnalysis,
    toggleMissingInfoOption,
    generateFollowUpBySelectedInfo,
    generateSimpleIntelligentFollowUp,
    generateIntelligentNeedsAnalysis,
    skipInfoCollection,
    // å»ºè®®åé¦ˆç›¸å…³æ–¹æ³•
    acceptSuggestion,
    negotiateSuggestion,
    cancelNegotiation,
    sendNegotiationRequest,
    rejectSuggestion,
    // è¿½é—®åé¦ˆç›¸å…³æ–¹æ³•
    acceptFollowUp,
    negotiateFollowUp,
    cancelFollowUpNegotiation,
    sendFollowUpNegotiationRequest,
    rejectFollowUp,
    confirmDirectSendToProblem,
    cancelDirectSend,
    prepareDirectSendCandidate,
    generateCustomerReplyForMessage,
    negotiateCustomerReplyCandidate,
    // æ™ºèƒ½è¿½é—®åé¦ˆç›¸å…³æ–¹æ³•
    acceptIntelligentFollowUp,
    negotiateIntelligentFollowUp,
    cancelIntelligentFollowUpNegotiation,
    sendIntelligentFollowUpNegotiationRequest,
    rejectIntelligentFollowUp,
    // æ–°å¢žï¼šèŠå¤©æ¨¡å¼ç›¸å…³çŠ¶æ€å’Œæ–¹æ³•
    chatMode,
    departmentModalVisible,
    emergencyModalVisible,
    switchChatMode,
    generateDepartmentContactSuggestion,
    handleEmergencyMode,
    closeModeModal,
    closeEmergencyModalOnly,
    // æ–°å¢žï¼šæµå¼æ˜¾ç¤ºç›¸å…³çŠ¶æ€
    thinkingContent,
    answerContent,
    isStreaming,
    // æ–°å¢žï¼šåä½œå·¥ä½œå°æµå¼çŠ¶æ€
    streamingMessage,
    streamingMessageContent,
    // æ–°å¢žï¼šAIæŽ¨ç†å¹²é¢„ç›¸å…³çŠ¶æ€å’Œæ–¹æ³•
    isPaused,
    pauseAI,
    resumeAI,
    adjustAI,
    // åŽŸæœ‰æ–¹æ³•
    sendProblemMessage,
    sendCustomerReplyToProblem,
    sendSolutionMessage,
    generateSuggestion,
    generateFollowUp,
    generateDepartmentContact,
    generateDepartmentContactOnly,
    chatWithAI,
    clearAiChatHistory,
    aiChatHistory,
    markContactInstructionSent,
    markCustomerReplyApplied,
    confirmSendResponse,
    cancelIteration,
    clearMessages: clearAllStates
  }
}